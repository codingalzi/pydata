{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 머신러닝 맛보기 3편"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주요 내용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 경사하강법 의미\n",
    "1. 그레이디언트 계산\n",
    "1. 경사하강법과 선형회귀\n",
    "1. 미니배치/확률적 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필수 모듈 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형대수에서 정의한 함수를 사용하기 위한 준비가 필요하며\n",
    "필요한 코드가 `pydata06_linear_algebra_basics.py` 파일에 저장되어 있다고 가정한다.\n",
    "\n",
    "먼저 파일을 다운로드한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pydata06_linear_algebra_basics.py',\n",
       " <http.client.HTTPMessage at 0x7f89b460b450>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "file_url = \"https://raw.githubusercontent.com/codingalzi/pydata/master/notebooks/pydata06_linear_algebra_basics.py\"\n",
    "file_name = \"pydata06_linear_algebra_basics.py\"\n",
    "urllib.request.urlretrieve(file_url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydata06_linear_algebra_basics as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 1: 경사하강법 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 데이터셋을 가장 잘 반영하는 최적의 수학적 모델을 찾으려 할 때 가장 기본적으로 사용되는 기법이\n",
    "**경사하강법**(gradient descent)이다.\n",
    "최적의 모델에 대한 기준은 학습법에 따라 다르지만, \n",
    "보통 학습된 모델의 오류를 최소화하도록 유도하는 기준을 사용한다. \n",
    "\n",
    "여기서는 선형회귀 모델을 학습하는 데에 기본적으로 사용되는 **평균 제곱 오차**(mean squared error, MSE)를\n",
    "최소화하기 위해 경사하강법을 적용하는 과정을 살펴본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 기본 아이디어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 $f:\\textbf{R}^n \\to \\textbf{R}$의 최댓값(최솟값)을 구하고자 한다.\n",
    "예를 들어, \n",
    "길이가 $n$인 실수 벡터를 입력받아 항목들의 제곱의 합을 계산하는 함수가 다음과 같다고 하자.\n",
    "\n",
    "$$\n",
    "f(\\mathbf x) = f(x_1, ..., x_n) = \\sum_{k=1}^{n} x_k^2 = x_1^2 + \\cdots x_n^2\n",
    "$$\n",
    "\n",
    "아래 코드에서 정의된 `sum_of_squares()`가 위 함수를 파이썬으로 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(v: LA.Vector) -> float:\n",
    "    \"\"\"\n",
    "    v 벡터에 포함된 원소들의 제곱의 합 계산\n",
    "    \"\"\"\n",
    "    return LA.dot(v, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 `sum_of_squares(v)`가 최대 또는 최소가 되는 벡터 `v`를 찾고자 한다.\n",
    "\n",
    "그런데 특정 함수의 최댓값(최솟값)이 존재한다는 것이 알려졌다 하더라도 실제로 최댓값(최솟값)\n",
    "지점을 확인하는 일은 일반적으로 매우 어렵고, 경우에 따라 불가능하다. \n",
    "따라서 보통 해당 함수의 그래프 위에 존재하는 임의의 점에서 시작한 후\n",
    "그레이디언트를 방향(반대 방향)으로 조금씩 이동하면서 최댓값(최솟값) 지점을 찾아가는\n",
    "**경사하강법**(gradient descent)을 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그레이디언트의 정의와 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 $f:\\textbf{R}^n \\to \\textbf{R}$가 vector $\\textbf{x}\\in \\textbf{R}$에서 \n",
    "미분 가능할 때 그레이디언트는 다음처럼 편미분으로 이루어진 벡터로 정의된다. \n",
    "\n",
    "$$\n",
    "\\nabla f(\\textbf{x}) =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial x_1} f(\\textbf{x}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial}{\\partial x_n} f(\\textbf{x})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "아래에서 왼편 그림은 $n=1$인 경우 2차원 상에서, 오른편 그림은 $n=2$인 경우에 3차원 상에서 \n",
    "그려지는 함수의 그래프와 특정 지점에서의 \n",
    "그레이디언트를 보여주고 있다. \n",
    "\n",
    "* 왼편 그림\n",
    "    * 그레이디언트는 접선(tangent line)의 기울기(slope)를 가리키는 미분값 $f'(x)$이다.\n",
    "    * 갈색 직선이 접선을 가리킨다.\n",
    "* 오른편 그림\n",
    "    * 그레이디언트는 편미분값으로 구성된 길이가 2인 벡터\n",
    "        $(\\frac{\\partial}{\\partial x_1} f(\\textbf{x}), \\frac{\\partial}{\\partial x_2} f(\\textbf{x}))$ 로 계산되며, 위쪽으로 향하는 파란색 화살표로 표시된다.\n",
    "    * 파란색 초평면(hyperplane)은 해당 지점에서 그래프와 접하는 평면을 보여준다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/Tangent-line.png\" alt=\"경사하강법\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/tangent_space-90.png\" alt=\"경사하강법\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            &#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>\n",
    "        </td>\n",
    "        <td>\n",
    "            &#60;출처\t&#62; <a href=\"\"https://github.com/pvigier/gradient-descent>pvigier: gradient-descent </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 경사하강법 작동 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법은 다음 과정을 반복하여 최댓값(최솟값) 지점을 찾아가는 것을 의미한다. \n",
    "\n",
    "* 해당 지점에서 그레이디언트(gradient)를 계산한다.\n",
    "* 계산된 그레이디언트의 방향(반대방향)으로 그레이디언트 크기의 일정 비율만큼 이동한다. \n",
    "\n",
    "아래 그림은 2차원 함수의 최솟값을 경사하강법으로 찾아가는 과정을 보여준다.\n",
    "최솟값은 해당 지점에서 구한 그레이디언트의 반대방향으로 조금씩 이동하는 방식으로 이루어진다. \n",
    "\n",
    "최솟값 지점에 가까워질 수록 그레이디언트는 점점 0벡터에 가까워진다. \n",
    "따라서 그레이디언트가 충분히 작아지면 최솟값 지점에 매우 가깞다고 판단하여 그 위치에서\n",
    "최솟값의 근사치를 구하여 활용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Gradient-Descent.gif\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"\"https://github.com/pvigier/gradient-descent>pvigier: gradient-descent </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주의사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법은 지역 최솟값(local minimum)이 없고 전역 최솟값(global mininum)이 존재할 경우 유용하게 활용된다.\n",
    "반면에 지역 최솟값이 존재할 경우 제대로 작동하지 않을 수 있기 때문에 많은 주의를 요한다. \n",
    "아래 그림은 출발점과 이동 방향에 따라 도착 지점이 전역 또는 지역 최솟점이 될 수 있음을 잘 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Gradient.png\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 2: 그레이디언트 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단변수 함수와 다변수 함수의 경우 그레이디언트 계산이 조금 다르다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단변수 함수의 도함수 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f$가 단변수 함수(1차원 함수)일 때, 점 $x$에서의 그레이디언트는 다음과 같이 구한다. \n",
    "\n",
    "$$\n",
    "f'(x) = \\lim_h \\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "즉, $f'(x)$ 는 $x$가 아주 조금 변할 때 $f(x)$가 변하는 정도, 즉\n",
    "함수 $f$의 $x$에서의 **미분값**이 된다.\n",
    "이때 함수 $f'$ 을 함수 $f$의 **도함수**라 부른다.\n",
    "\n",
    "예를 들어, 제곱 함수 $f(x) = x^2$에 해당하는 `square()`가 아래와 같이 주어졌다고 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x: float) -> float:\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면 `square()`의 도함수 $f'(x) = 2x$는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(x: float) -> float:\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이와 같이 미분함수가 구체적으로 주어지는 경우는 많지 않다.\n",
    "따라서 여기서는 많은 경우 충분히 작은 $h$에 대한 함수의 변화율을 측정하면\n",
    "미분값의 근사치를 사용할 수 있다는 사실을 확인하고자 한다.\n",
    "\n",
    "아래 그림은 $h$가 작아질 때 \n",
    "두 점 $f(x+h)$ 와 $f(x)$ 지나는 직선이 변하는 과정을 보여준다.\n",
    "$h$가 0에 수렴하면 최종적으로 점 $x$에서의 접선이 되고\n",
    "미분값 $f'(x)$는 접선의 기울기가 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Derivative.gif\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 임의의 단변수 함수에 대해 함수 변화율을 구해주는 함수를 간단하게 구현한 것이다.\n",
    "          \n",
    "* `Callable`은 함수에 대한 자료형을 가리킨다. \n",
    "    * `Callable[[float], float]`: 부동소수점을 하나 받아 부동소수점을 계산하는 함수들의 클래스를 가리킴.\n",
    "* `different_quotient()` 함수가 사용하는 세 인자와 리턴값의 자료형은 다음과 같다. \n",
    "    * 미분 대상 함수: `f: Callable[[float], float]`\n",
    "    * 미분 위치: `x: float`\n",
    "    * 인자가 변하는 정도: `h: float`\n",
    "    * 리턴값(`float`): $f'(x)$의 근사치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def difference_quotient(f: Callable[[float], float],\n",
    "                        x: float,\n",
    "                        h: float) -> float:\n",
    "    \"\"\"\n",
    "    함수 f의 x에서의 미분값 근사치 계산\n",
    "    f: 미분 대상 함수\n",
    "    x: 인자\n",
    "    h: x가 변하는 정도\n",
    "    \"\"\"\n",
    "    \n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 `square()`의 도함수 `derivative()` 를 `difference_quotient()`를 \n",
    "이용하여 충분히 근사적으로 구현할 수 있음을 그래프로 보여준다. \n",
    "근사치 계산을 위해 `h=0.001` 를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "h = 0.001\n",
    "xs = range(-10, 11)\n",
    "\n",
    "actuals = [derivative(x) for x in xs]\n",
    "estimates = [difference_quotient(square, x, h) for x in xs]\n",
    "\n",
    "plt.title(\"Actual Derivatives vs. Estimates\")\n",
    "# 실제 도함수 그래프(빨간색 점)\n",
    "plt.plot(xs, actuals, 'r.', label='Actual') \n",
    "# 근사치 그래프(검은색 +)\n",
    "plt.plot(xs, estimates, 'k+', label='Estimates')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다변수 함수의 그레이디언트 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다변수 함수의 그레이디언트는 매개변수 각가에 대한 **편도함수**(partial derivative)로\n",
    "구성된 벡터로 구성된다.\n",
    "예를 들어, $i$번째 편도함수는 $i$번째 매개변수를 제외한 다른 모든 매개변수를 고정하는 \n",
    "방식으로 계산된다. \n",
    "\n",
    "$f$가 다변수 함수(다차원 함수)일 때, 점 $\\mathbf{x}$에서의 $i$번째 도함수는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_i}f(\\mathbf x) = \\lim_h \\frac{f(\\mathbf{x}_h) - f(\\mathbf x)}{h}\n",
    "$$\n",
    "\n",
    "여기서 $\\mathbf{x}_h$는 $\\mathbf x$의 $i$번째 항목에 $h$를 더한 벡터를 가리킨다.\n",
    "즉, $\\frac{\\partial}{\\partial x_i} f(\\mathbf x)$ 는 $x_i$가 아주 조금 변할 때 \n",
    "$f(\\mathbf x)$가 변하는 정도, 즉\n",
    "함수 $f$의 $x$에서의 $i$번째 **편미분값**이 된다.\n",
    "이때 함수 $\\frac{\\partial}{\\partial x_i}f$ 를 함수 $f$의 $i$번째 **편도함수**라 부른다.\n",
    "\n",
    "아래 코드에서 정의된 `partial_difference_quotient`는 주어진 다변수 함수의 $i$번째\n",
    "편도함수의 근사치를 계산해주는 함수이다.\n",
    "사용되는 매개변수는 `difference_quotient()` 함수의 경우와 거의 같다.\n",
    "다만 `i`번째 편도함수의 근사치를 지정하기 위해 `i`번째 매개변수에만 `h`가 더해짐에 주의하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f: Callable[[LA.Vector], float],\n",
    "                                v: LA.Vector,\n",
    "                                i: int,\n",
    "                                h: float) -> float:\n",
    "    \"\"\"\n",
    "    함수 f의 v에서의 i번째 편미분값 근사치 계산\n",
    "    f: 편미분 대상 함수\n",
    "    v: 인자 벡터\n",
    "    i: i번째 인자를 가리킴\n",
    "    h: 인자 v_i가 변하는 정도\n",
    "    \"\"\"\n",
    "    \n",
    "    # v_i에 대해서만 h 더한 벡터\n",
    "    w = [v_j + (h if j == i else 0) for j, v_j in enumerate(v)]\n",
    "\n",
    "    return (f(w) - f(v)) / h    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 `estimate_gradient()` 함수는 편미분 근사치를 이용하여 \n",
    "그레이디언트의 근사치에 해당하는 벡터를 리스트로 계산한다. \n",
    "근사치 계산에 사용된 `h`의 기본값은 0.0001이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f: Callable[[LA.Vector], float],\n",
    "                      v: LA.Vector,\n",
    "                      h: float = 0.0001):\n",
    "    return [partial_difference_quotient(f, v, i, h) for i in range(len(v))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주의사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그레이디언트를 두 개의 함수값의 차이를 이용하여 근사치로 계산하는 방식은 계산 비용이 크다.\n",
    "벡터 `v`의 길이가 $n$이면 `estimate_gradient()` 함수를 호출할 때마다\n",
    "`f`를 $2n$ 번 호출해야 하기 때문이다. \n",
    "따라서 앞으로는 그레이디언트 함수가 수학적으로 쉽게 계산되는 경우만을 \n",
    "사용하여 경사하강법의 용도를 살펴볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 3: 경사하강법과 선형회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 정의한 제곱함수 `sum_of_squares()` 는 `v`가 0 벡터일 때 가장 작은 값을 갖는다. \n",
    "이 사실을 경사하강법을 이용하여 확인해보자. \n",
    "\n",
    "먼저, `sum_of_squares()` 함수의 그레이디언트는 다음과 같이 정의된다. \n",
    "\n",
    "$$\n",
    "\\nabla f(\\textbf{x}) =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial x_1} f(\\textbf{x}) \\\\\n",
    "    \\frac{\\partial}{\\partial x_2} f(\\textbf{x})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "아래 코드는 리스트를 이용하여 그레이디언트를 구현하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares_gradient(v: LA.Vector) -> LA.Vector:\n",
    "    return [2 * v_i for v_i in v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 임의의 지점(`v`)에서 계산된 그레이디언트에 스텝(step)이라는 특정 상수를 곱한 값을\n",
    "더해 새로운 지점을 계산하는 함수를 구현한다.\n",
    "\n",
    "* `addV`, `scalar_multV`, `distance` 등은 선형대수 부분에서 정의한 \n",
    "    `pydata06_linear_algebra_basics.py` 모듈에서 가져온다.\n",
    "* `addV(v, step)`: 스텝사이즈가 음수이면 그레이디언트가 가리키는 방향의 반대방향으로 지정된 비율만큼 움직인 벡터."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# v에서의 그레이디언트가 가리키는 방향의 반대 방향으로 스텝이 지정한 비율만큼 v값 이동\n",
    "def gradient_step(v: LA.Vector, gradient: LA.Vector, step_size: float) -> LA.Vector:\n",
    "    step = LA.scalar_multV(step_size, gradient)\n",
    "    new_V = LA.subtractV(v, step)   # 그레이디언트의 반대방향으로 이동\n",
    "    return new_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 임의의 지점에서 출발하여 `gradient_step`을 충분히 반복하면\n",
    "제곱함수의 최솟점에 충분히 가깝게 근사할 수 있음을 아래 코드가 보여준다.\n",
    "실제로 전역 최솟값의 위치인 원점에 매우 가깝게 접근한다.\n",
    "\n",
    "* `random.seed(42)`: 실행할 때마다 동일한 결과를 보장해준다. 사용하지 않으면 매번 다른 결과가 나옴.\n",
    "* `grad`: 이동할 때마다 계산된 그레이디언트. 최종적으로 0 벡터에 가까운 값을 갖게 됨.\n",
    "* `if epoch%100 == 0`: 위치 이동을 100번 할 때마다 현재 위치 확인\n",
    "* `epoch`(에포크): 여기서는 그레이디언트 계산 횟수. 즉, 이동횟수를 가리킴.\n",
    "* `step_size=0.01`: 그레이디언트 반대 방향, 즉, 최솟값 지점을 향해 이동할 때 사용되는 크기 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [2.732765249774521, -9.309789197635727, -4.409425359965263]\n",
      "100 [0.3624181137897112, -1.2346601088642208, -0.5847760329896551]\n",
      "200 [0.048063729299005625, -0.16374007531854073, -0.07755273779298358]\n",
      "300 [0.006374190434279768, -0.02171513607091833, -0.010285009644527733]\n",
      "400 [0.0008453423045827667, -0.002879851701919325, -0.0013639934114305214]\n",
      "500 [0.00011210892101281368, -0.00038192465375128993, -0.00018089220046728499]\n",
      "600 [1.4867835316559317e-05, -5.065067796575345e-05, -2.3989843290795995e-05]\n",
      "700 [1.971765716798422e-06, -6.717270417586384e-06, -3.1815223632100903e-06]\n",
      "800 [2.6149469369030636e-07, -8.908414196052704e-07, -4.2193208287814765e-07]\n",
      "900 [3.467931014604295e-08, -1.1814299344070243e-07, -5.5956445449048146e-08]\n",
      "\n",
      "----\n",
      "\n",
      "그레이디언트의 최종 값: [9.57758165411209e-09, -3.262822016281278e-08, -1.5453808714914104e-08]\n",
      "v의 최후 위치와 최솟점 사이의 거리: 1.830234305038648e-08\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# 임의로 선택된 출발점 좌표\n",
    "v = [random.uniform(-10, 10) for i in range(3)]\n",
    "\n",
    "# gradient_step 1000번 반복\n",
    "for epoch in range(1000):\n",
    "    grad = sum_of_squares_gradient(v)  # 그레이디언트 계산\n",
    "    \n",
    "    v = gradient_step(v, grad, 0.01)  # 좌표 업데이트\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        print(epoch, v)\n",
    "\n",
    "print(\"\\n----\\n\")        \n",
    "print(f\"그레이디언트의 최종 값: {grad}\")\n",
    "print(f\"v의 최후 위치와 최솟점 사이의 거리: {LA.distance(v, [0, 0, 0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 에포크와 스텝 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 사용된 코드에서 에포크(epoch)는 이동 횟수를 가리키며, \n",
    "에포크를 크게 하면 최솟값 지점에 보다 가까워진다.\n",
    "하지만 항상 수렴하는 방향으로 이동하는 것은 아니다.\n",
    "하지만 여기서는 스텝 크기를 너무 크게 지정하지만 않으면 항상 최솟값에 수렴하는 \n",
    "볼록함수만 다룬다. \n",
    "스텝 크기에 따른 수렴속도는 다음과 같다.\n",
    "\n",
    "* 스텝 크기 크게: 수렴 속도가 빨라진다.\n",
    "* 스텝 크기 작게: 수렴 속도가 느려진다.\n",
    "\n",
    "하지만 다루는 함수에 따른 적당한 스텝의 크기가 달라지며,\n",
    "보통 여러 실혐을 통해 정해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법을 이용하여 주어진 데이터들의 분포에 대한 선형 모델을 구하는 방법을\n",
    "**선형회귀**(linear regression)라 부른다. \n",
    "\n",
    "먼저 $y = f(x) = 5 + 20x$ 일차함수의 그래프에 해당하는 데이터를 구한다. \n",
    "여기서 $x$는 -0.5에서 0.5 사이에 있는 100개의 값으로 주어지며,\n",
    "$y$값에 약간의 잡음(가우시안 잡음)이 추가된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x는 -0.5에서 0.5 사이\n",
    "xs = [x/100 for x in range(-50, 50)]\n",
    "\n",
    "# 약간의 잡음 추가 (가우시안 잡음)\n",
    "error = [random.randrange(-100,100)/100 for _ in range(-50, 50)]\n",
    "\n",
    "# y = 5 + 20*x + 가우시안 잡음\n",
    "ys = [20*x + 5 + e for x, e in zip(xs, error)]\n",
    "\n",
    "# (x,y) 좌표값들의 리스트\n",
    "inputs = list(zip(xs, ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 프로그램은 잡음이 포함되어 직선으로 그려지지 않는 \n",
    "데이터의 분포를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xs, ys, 'r.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 위 그래프를 선형적으로 묘사하는 함수를 경사하강법을 이용하여 구현한다.\n",
    "구현 대상은 아래 모양의 함수이다. \n",
    "\n",
    "$$\n",
    "\\hat y = f(x) = \\theta_0 + \\theta_1\\cdot x\n",
    "$$\n",
    "\n",
    "* $\\theta_0$: 절편\n",
    "* $\\theta_1$: 기울기\n",
    "* $\\hat y$: $y$에 대한 예측값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그래프를 가장 잘 묘사하는 직선을 구해야 한다.\n",
    "즉, 적절한 $\\theta_0$와 $\\theta_1$을 구해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기준"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법을 적용하려면 최소화 대상인 함수를 지정해야 한다.\n",
    "여기서는 예측치 $\\hat y$와 실제 $y$ 사이의 오차의 **평균제곱오차**(mean squared error, MSE)를\n",
    "계산하는 함수를 사용한다.\n",
    "\n",
    "$$\n",
    "\\textrm{MSE}(\\theta_0, \\theta_1) = \\sum_{y \\in ys} (\\hat y - y)^2\n",
    "= \\sum_{x \\in \\textrm{xs}} (\\theta_0 + \\theta_1\\cdot x - y)^2\n",
    "$$\n",
    "\n",
    "즉, MSE를 최소로 하는 $\\theta_0, \\theta_1$을 구해야 한다.\n",
    "MSE의 그레이디언트는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_0} \\textrm{MSE}(\\theta_0, \\theta_1)\n",
    "= \\sum_{x \\in \\textrm{xs}} 2(\\hat y - y)\\, ,\n",
    "\\qquad\n",
    "\\frac{\\partial}{\\partial \\theta_1} \\textrm{MSE}(\\theta_0, \\theta_1)\n",
    "= \\sum_{x \\in \\textrm{xs}} 2(\\hat y - y) x\\\n",
    "$$\n",
    "\n",
    "아래 코드는 특정 $x$에 대한 실제 값 $y$와 예측치 $\\hat y$ 사이의 제곱오차와\n",
    "제곱오차의 그레이디언트를 계산한다.\n",
    "\n",
    "* `intercept`: $\\theta_0$ (절편)\n",
    "* `slope`: $\\theta_1$ (기울기)\n",
    "* `predicted`: $\\hat y$\n",
    "* `error`: $(\\hat y - y)$\n",
    "* `grad`: $(2(\\hat y - y), 2(\\hat y - y) x)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_gradient(x: float, y: float, theta: LA.Vector) -> LA.Vector:\n",
    "    intercept, slope = theta          # 절편과 기울기\n",
    "    predicted = intercept + slope * x  # 예측값\n",
    "    error = (predicted - y)            # 오차\n",
    "    squared_error = error ** 2         # 제곱 오차\n",
    "    grad = [2 * error, 2 * error * x]  # (특정 x에 대한) 제곱 오차의 그레이디언트\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 임의의 $\\theta = (\\theta_0, \\theta_1)$로 시작한 후에\n",
    "경사하강법으로 평균제곱오차의 최솟값 지점을 구할 수 있다.\n",
    "\n",
    "* `vector_mean`: 벡터 항목들의 평균값 계산\n",
    "* `epoch`(에포크): 5000회 반복\n",
    "* `learning_rate`: 스텝 크기. 보통 **학습률**이라 부름.\n",
    "\n",
    "아래 코드에서 사용한 학습률은 0.001이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.5498750271006548, -0.348065046729531]\n",
      "500 [2.8916275137318697, 1.2779917858845682]\n",
      "1000 [4.161340996373953, 2.78438692444515]\n",
      "1500 [4.63252303459113, 4.174144289893471]\n",
      "2000 [4.809879153546342, 5.454184661194932]\n",
      "2500 [4.878918399660604, 6.6323964362145]\n",
      "3000 [4.907842395609395, 7.716596269085061]\n",
      "3500 [4.921739999085596, 8.714181270348421]\n",
      "4000 [4.929854101282013, 9.632032625512535]\n",
      "4500 [4.935602366135786, 10.476509096849366]\n",
      "최종 기울기: 11.252\n",
      "최종 절편: 4.940\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(5000):\n",
    "    # 평균 제곱 오차 계산 (전체 훈련 데이터 대상)\n",
    "    grad = LA.vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "    \n",
    "    # theta 값 업데이트. 그레이디언트 반대 방향으로 지정된 학습률 비율로 이동\n",
    "    theta = gradient_step(theta, grad, learning_rate)\n",
    "    \n",
    "    # 500번에 한 번 학습과정 확인\n",
    "    if epoch % 500 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 최종 기울기가 11.4 정로 애초에 사용한 20과 차이가 크다.\n",
    "이유는 학습률이 너무 낮아서 5000번의 에포크가 충북한 학습을 위해 부족했기 때문이다.\n",
    "이에 대한 해결책은 보통 두 가지이다. \n",
    "\n",
    "* 첫째: 학습률 키우기\n",
    "* 둘째: 에포크 키우기\n",
    "\n",
    "아래 코드는 먼저 에포크를 네 배 늘린 결과를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.5453216196333258, -0.8846541803415932]\n",
      "1000 [4.3071961393797, 2.3344318287971526]\n",
      "2000 [4.827835026420881, 5.073878601499392]\n",
      "3000 [4.908792115820234, 7.394749506518404]\n",
      "4000 [4.928733502968913, 9.359603724571762]\n",
      "5000 [4.939051515502755, 11.022864887660349]\n",
      "6000 [4.9468993258866565, 12.430800212519703]\n",
      "7000 [4.953422706517003, 13.62260111714172]\n",
      "8000 [4.958928503638361, 14.631446280141436]\n",
      "9000 [4.96358691111091, 15.485421540772457]\n",
      "10000 [4.967529901914979, 16.208301288794544]\n",
      "11000 [4.97086755619471, 16.82021026697407]\n",
      "12000 [4.973692834861904, 17.338183828260746]\n",
      "13000 [4.976084398412164, 17.776642193387854]\n",
      "14000 [4.97810882798133, 18.147791905104715]\n",
      "15000 [4.979822483153499, 18.46196565445812]\n",
      "16000 [4.9812730715626445, 18.727909939652793]\n",
      "17000 [4.982500977134955, 18.95302856580554]\n",
      "18000 [4.98354038437498, 19.14358876454719]\n",
      "19000 [4.98442023005287, 19.30489567177664]\n",
      "최종 기울기: 19.441\n",
      "최종 절편: 4.985\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(20000):\n",
    "    # 평균 제곱 오차 계산 (전체 훈련 데이터 대상)\n",
    "    grad = LA.vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "    \n",
    "    # theta 값 업데이트. 그레이디언트 반대 방향으로 지정된 학습률 비율로 이동\n",
    "    theta = gradient_step(theta, grad, learning_rate)\n",
    "    \n",
    "    # 1000번에 한 번 학습과정 확인\n",
    "    if epoch % 1000 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반면에 아래 코드는 학습률을 0.01로 키웠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.7285600934134986, 0.6423040193614589]\n",
      "500 [4.942726752245414, 11.692237628170421]\n",
      "1000 [4.9691293209021055, 16.501531174187775]\n",
      "1500 [4.980523087447129, 18.590411261023977]\n",
      "2000 [4.985471879329149, 19.497700269800106]\n",
      "2500 [4.987621349042239, 19.891774276545842]\n",
      "3000 [4.988554954689417, 20.062937292178695]\n",
      "3500 [4.988960459125764, 20.137280632310304]\n",
      "4000 [4.989136586860627, 20.16957109062855]\n",
      "4500 [4.989213086588395, 20.183596202986426]\n",
      "최종 기울기: 20.190\n",
      "최종 절편: 4.989\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(5000):\n",
    "    # 평균 제곱 오차 계산 (전체 훈련 데이터 대상)\n",
    "    grad = LA.vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "    \n",
    "    # theta 값 업데이트. 그레이디언트 반대 방향으로 지정된 학습률 비율로 이동\n",
    "    theta = gradient_step(theta, grad, learning_rate)\n",
    "    \n",
    "    # 500번에 한 번 학습과정 확인\n",
    "    if epoch % 500 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과를 비교했을 때 학습률을 키우는 것이 보다 효과적이다. \n",
    "최종적으로 구해진 기울기와 절편을 이용하여 처음에 주어진 데이터의 분포를\n",
    "선형적으로 학습한 1차함수의 그래프는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 예측치\n",
    "zs = [intercept + slope *x for x in xs]\n",
    "\n",
    "# 실제 데이터 분포\n",
    "plt.plot(xs, ys, 'r.', label='Actuals')\n",
    "# 예측치 그래프\n",
    "plt.plot(xs, zs, 'b-', label='Estimates')\n",
    "\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 4: 미니배치/확률적 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 사용한 경사하강법은 주어진 데이터셋 전체를 대상으로 평균제곱오차와 그레이디언트를 계산하였다.\n",
    "이런 방식을 **배치 경사하강법**이라 부른다.\n",
    "\n",
    "**주의**: 배치(batch)는 원래 하나의 묶음을 나타내지만 여기서는 주어진 (훈련) 데이터셋 전체를 가리키는\n",
    "의미로 사용된다.\n",
    "\n",
    "그런데 사용된 데이터셋의 크기가 100이었기 때문에 계산이 별로 오래 걸리지 않았지만,\n",
    "데이터셋이 커지면 그러한 계산이 매우 오래 걸릴 수 있다.\n",
    "실전에서 사용되는 데이터셋의 크기는 몇 만에서 수십억까지 다양하며, \n",
    "그런 경우에 적절한 학습률을 찾는 과정이 매우 오래 걸릴 수 있다.\n",
    "\n",
    "데이터셋이 매우 큰 경우에는 따라서 아래 두 가지 방식을 추천한다.\n",
    "\n",
    "* 미니배치 경사하강법\n",
    "* 확률적 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미니배치 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 경사하강법과는 달리 미니매치 경사하강법(mini-batch gradient descent)은\n",
    "일정 크기의 훈련 데이터를 대상으로 그레이디언트를 계산한다.\n",
    "\n",
    "예를 들어, 전체 데이터셋의 크기가 1000이고 미니배치의 크기를 10이라 하면,\n",
    "배치 경사하강법에서는 하나의 에포크를 돌 때마다 한 번 MSE와 그레이디언트를 계산하였지만\n",
    "미니배치 경사하강법에서는 10개의 데이터를 확인할 때마다 MSE와 그레이디언트를 계산하여\n",
    "하나의 에포크를 돌 때마다 총 100번 기울기와 절편을 업데이트한다. \n",
    "\n",
    "아래 코드에서 정의된 `minibatches()` 함수는 호출될 때마다\n",
    "`batch_size`로 지정된 크기의 데이터 세트를 전체 데이터셋에서\n",
    "선택해서 내준다.\n",
    "\n",
    "데이터를 선택하는 방식은 다음과 같다.\n",
    "\n",
    "* `minibatches()` 함수는 제너레이터이다. \n",
    "    즉, 요구될 때마다 항목을 생성하여 리턴하는 함수이다. \n",
    "* 전체 데이터셋을 인덱스 기준으로 미니배치 크기(`batch_size`) 만큼씩 구분하여\n",
    "    `batch_starts` 리스트에 저장한다.\n",
    "* 섞기(`shuffle`) 옵션이 `True`일 경우, `batch_starts`에 포함된 항목들의 순서를 무작위로 섞는다.\n",
    "* 최종적으로 `batch_starts` 에 포함된 인데스를 기준으로 지정된 미니배치 크기만큼의 \n",
    "    데이터를 다음 MSE, 그레이디언트 계산용으로 내어 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Iterator\n",
    "\n",
    "# 제너레이터 함수 정의\n",
    "def minibatches(dataset: List[float],\n",
    "                batch_size: int,\n",
    "                shuffle: bool = True) -> Iterator[List[float]]:\n",
    "    \"\"\"\n",
    "    dataset: 전체 데이터셋\n",
    "    batch_size: 미니배치 크기\n",
    "    shuffle: 섞기 옵션\n",
    "    리턴값: 이터레이터\n",
    "    \"\"\"\n",
    "\n",
    "    # 0번 인덱스부터 시작하여, batch_size 배수 번째에 해당하는 인덱스만 선택\n",
    "    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
    "    \n",
    "    # shuffle 옵션이 참이면 인덱스 섞기\n",
    "    if shuffle: random.shuffle(batch_starts)\n",
    "\n",
    "    # batch_starts에  포함된 인덱스를 기준으로 해서 미니배치 크기만큼씩 선택해서 \n",
    "    # 다음 MSE와 그레이디언트 계산에 필요한 훈련 데이터 세트를 지정함.\n",
    "    for start in batch_starts:\n",
    "        end = start + batch_size\n",
    "        yield dataset[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 미니배치 경사하강법을 이전에 사용했던 데이터에 대해 적용한다.\n",
    "학습률(`learning_rate`)을 0.001로 하면 학습이 제대로 이루어지지 않는다.\n",
    "에포크 수를 키우거나 합습률을 크게 해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.14700601161032584, -0.850324957636322]\n",
      "100 [3.0383003005334195, 0.81879593562066]\n",
      "200 [4.21458258079334, 2.363808765349366]\n",
      "300 [4.65037766647808, 3.7887387104243513]\n",
      "400 [4.814916905651929, 5.100937311842601]\n",
      "500 [4.880054707558669, 6.308581062740375]\n",
      "600 [4.906941797916403, 7.419765388918784]\n",
      "700 [4.921300749140654, 8.442074919728974]\n",
      "800 [4.928451522266928, 9.382601070352939]\n",
      "900 [4.934261548296817, 10.247863490712117]\n",
      "최종 기울기: 11.036\n",
      "최종 절편: 4.939\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률 지정\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 1000번의 에포크\n",
    "for epoch in range(1000):\n",
    "    # 미니배치의 크기를 20으로 지정함\n",
    "    # 따라서 한 번의 에포크마다 5번 MSE와 그레이디언트 계산 후 기울기와 절편 업데이트\n",
    "    # 섞기 옵션 사용\n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = LA.vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, learning_rate)\n",
    "    # 100개의 에포크가 지날 때마다 학습 내용 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률을 0.01로 키우면 좋은 결과가 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1.237825277922001, 0.7249192271332124]\n",
      "100 [4.952368308578804, 11.798388090708]\n",
      "200 [4.970608256534244, 16.578080917985734]\n",
      "300 [4.983392717759511, 18.637033407981054]\n",
      "400 [4.986056813379833, 19.523825735584676]\n",
      "500 [4.987595092175629, 19.90597277649338]\n",
      "600 [4.988266095521663, 20.070316968833907]\n",
      "700 [4.989040195003638, 20.14103697686222]\n",
      "800 [4.989672228772879, 20.17161547560229]\n",
      "900 [4.988703699118148, 20.184765698131514]\n",
      "최종 기울기: 20.190\n",
      "최종 절편: 4.989\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률 지정\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 1000번의 에포크\n",
    "for epoch in range(1000):\n",
    "    # 미니배치의 크기를 20으로 지정함\n",
    "    # 따라서 한 번의 에포크마다 5번 MSE와 그레이디언트 계산 후 기울기와 절편 업데이트\n",
    "    # 섞기 옵션 사용\n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = LA.vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, learning_rate)\n",
    "    # 100개의 에포크가 지날 때마다 학습 내용 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률을 0.01로 두고 에포크를 3000으로 늘려도 성능이 그렇게 좋아지지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.43549987222961334, 0.26254473969499653]\n",
      "300 [4.978902536543744, 18.599116202912136]\n",
      "600 [4.988010821486811, 20.067010548132913]\n",
      "900 [4.989101560704833, 20.18451199354063]\n",
      "1200 [4.989313445216165, 20.193984886037637]\n",
      "1500 [4.98927937941379, 20.195015565259936]\n",
      "1800 [4.988570103335046, 20.195105271575724]\n",
      "2100 [4.989706094445955, 20.194956727051277]\n",
      "2400 [4.990361301352804, 20.194971623699445]\n",
      "2700 [4.989683544148361, 20.19504893855309]\n",
      "최종 기울기: 20.194\n",
      "최종 절편: 4.989\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률 지정\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 1000번의 에포크\n",
    "for epoch in range(3000):\n",
    "    # 미니배치의 크기를 20으로 지정함\n",
    "    # 따라서 한 번의 에포크마다 5번 MSE와 그레이디언트 계산 후 기울기와 절편 업데이트\n",
    "    # 섞기 옵션 사용\n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = LA.vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, learning_rate)\n",
    "    # 100개의 에포크가 지날 때마다 학습 내용 출력\n",
    "    if epoch % 300 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 과정을 살펴보면 기울기가 20.195 정도에서 더 이상 좋아지지 않는다.\n",
    "이런 경우 특별히 더 좋은 결과를 얻을 수 없다는 것을 의미한다.\n",
    "사실, 데이터셋을 지정할 때 가우시안 잡음을 추가하였기에 완벽한 선형함수를 찾는 것은 애초부터 불가능하다.\n",
    "따라서 위 결과를 미니배치 경사하강법을 사용한 선형회귀로 얻을 수 있는 최선으로 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 확률적 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률적 경사하강법(stochastic gradient descent, SGD)은\n",
    "미니배치의 크기가 1인 미니배치 경사하강법을 가리킨다.\n",
    "즉, 하나의 데이터를 학습할 때마다 그레이디언트를 계산하여 기울기와 절편을 업데이트 한다. \n",
    "\n",
    "아래 코드는 주어진 데이터셋을 대상로 SGD를 적용하는 방식을 보여준다.\n",
    "학습률을 0.001로 했음에도 불구하여 1000번의 에포크를 반복한 후에 이전에\n",
    "0.01을 사용했던 경우와 거의 동일한 결과를 얻는다는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.550382182232026, 0.9895536437047407]\n",
      "100 [5.032490886155949, 16.554658325069674]\n",
      "200 [4.997519092010245, 19.51236265820966]\n",
      "300 [4.990918695543546, 20.070584404084553]\n",
      "400 [4.989672971014022, 20.175940275546683]\n",
      "500 [4.9894378594398585, 20.19582459523417]\n",
      "600 [4.9893934857031255, 20.19957745840122]\n",
      "700 [4.989385110834649, 20.20028575429328]\n",
      "800 [4.989383530205495, 20.200419434379114]\n",
      "900 [4.989383231885758, 20.20044466446389]\n",
      "최종 기울기: 20.200\n",
      "최종 절편: 4.989\n"
     ]
    }
   ],
   "source": [
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 에포크는 1000\n",
    "for epoch in range(1000):\n",
    "    for x, y in inputs:\n",
    "        grad = linear_gradient(x, y, theta)\n",
    "        theta = gradient_step(theta, grad, learning_rate)\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률을 0.01로 하면 결과가 오히려 좀 더 나빠진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [6.8116974087303825, 2.3020744483619793]\n",
      "100 [4.990193170861476, 20.26103005086055]\n",
      "200 [4.990192825899716, 20.26103222379059]\n",
      "300 [4.990192825899663, 20.26103222379092]\n",
      "400 [4.990192825899663, 20.26103222379092]\n",
      "500 [4.990192825899663, 20.26103222379092]\n",
      "600 [4.990192825899663, 20.26103222379092]\n",
      "700 [4.990192825899663, 20.26103222379092]\n",
      "800 [4.990192825899663, 20.26103222379092]\n",
      "900 [4.990192825899663, 20.26103222379092]\n",
      "최종 기울기: 20.261\n",
      "최종 절편: 4.990\n"
     ]
    }
   ],
   "source": [
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 에포크는 1000\n",
    "for epoch in range(1000):\n",
    "    for x, y in inputs:\n",
    "        grad = linear_gradient(x, y, theta)\n",
    "        theta = gradient_step(theta, grad, learning_rate)\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 살펴본 것에 따르면 확률적 경사하강법의 성능이 가장 좋았다.\n",
    "하지만 이것은 경우에 따라 다르다.\n",
    "아래 그림은 각각의 방식에서 절편과 기울기가 학습되는 과정을 잘 보여준다.\n",
    "\n",
    "- 배치학습: 수렴값에 진동 없이 가장 빠르게 접근.\n",
    "- 미니배치 학습: 중간 속도로 접근함. 수렴값 근처에서 진동이 어느 정도 있음.\n",
    "- 확률적 경사 하강법: 매우 변화가 심함. 극한값 근처에서 여전히 진동이 심함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml2/master/slides/images/ch04/homl04-05.png\" width=\"500\"/></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
