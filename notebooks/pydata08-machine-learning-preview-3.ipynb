{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 머신러닝 맛보기 3편"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주요 내용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 경사하강법 의미\n",
    "1. 그레이디언트 계산\n",
    "1. 경사하강법과 선형회귀\n",
    "1. 미니배치/확률적 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 준비사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형대수에서 정의한 함수를 사용하기 위한 준비가 필요하며\n",
    "필요한 코드가 `pydata06_linear_algebra_basics.py` 파일에 저장되어 있다고 가정한다.\n",
    "\n",
    "먼저 파일을 다운로드한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pydata06_linear_algebra_basics.py',\n",
       " <http.client.HTTPMessage at 0x7ff3f578b700>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "file_url = \"https://raw.githubusercontent.com/codingalzi/pydata/master/notebooks/pydata06_linear_algebra_basics.py\"\n",
    "file_name = \"pydata06_linear_algebra_basics.py\"\n",
    "urllib.request.urlretrieve(file_url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydata06_linear_algebra_basics as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 데이터셋을 가장 잘 반영하는 최적의 수학적 모델을 찾으려 할 때 가장 기본적으로 사용되는 기법이\n",
    "**경사하강법**(gradient descent)이다.\n",
    "최적의 모델에 대한 기준은 학습법에 따라 다르지만, \n",
    "보통 학습된 모델의 오류를 최소화하도록 유도하는 기준을 사용한다. \n",
    "\n",
    "여기서는 선형회귀 모델을 학습하는 데에 기본적으로 사용되는 **평균 제곱 오차**(mean squared error, MSE)를\n",
    "최소화하기 위해 경사하강법을 적용하는 과정을 살펴본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 기본 아이디어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 $f:\\mathbf{R}^n \\to \\mathbf{R}$의 최솟값 지점을 구하고자 한다.\n",
    "예를 들어, \n",
    "길이가 $n$인 실수 벡터를 입력받아 항목들의 제곱의 합을 계산하는 함수가 다음과 같다고 하자.\n",
    "\n",
    "$$\n",
    "f(\\mathbf v) = f(v_1, ..., v_n) = \\sum_{k=1}^{n} v_k^2 = v_1^2 + \\cdots v_n^2\n",
    "$$\n",
    "\n",
    "아래 코드에서 정의된 `sum_of_squares()`가 위 함수를 파이썬으로 구현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__참고:__ `LA.Vector` 는 `typing.List[float]`, 즉 부동소수점들의 리스트 자료형을 가리킴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(v: LA.Vector) -> float:\n",
    "    \"\"\"\n",
    "    v 벡터에 포함된 원소들의 제곱의 합 계산\n",
    "    \"\"\"\n",
    "    return LA.dot(v, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 `sum_of_squares(v)`가 최대 또는 최소가 되는 벡터 `v`를 찾고자 한다.\n",
    "\n",
    "그런데 특정 함수의 최솟값이 존재한다는 것이 알려졌다 하더라도 실제로 최솟값\n",
    "지점을 확인하는 일은 일반적으로 매우 어렵고, 경우에 따라 불가능하다. \n",
    "따라서 보통 해당 함수의 그래프 위에 존재하는 임의의 점에서 시작한 후\n",
    "그레이디언트 반대 방향으로 조금씩 이동하면서 최솟값 지점을 찾아가는\n",
    "**경사하강법**(gradient descent)을 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그레이디언트의 정의와 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 $f:\\mathbf{R}^n \\to \\mathbf{R}$가 점 $\\mathbf{v}\\in \\mathbf{R}^n$에서 \n",
    "미분 가능할 때 $\\mathbf{v}$에서 $f$의 __그레이디언트__는 다음처럼 편미분으로 이루어진 벡터로 정의된다. \n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{v}) =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial v_1} f(\\mathbf{v}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial}{\\partial v_n} f(\\mathbf{v})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "아래에서 왼편 그림은 $n=1$인 경우 2차원 상에서, 오른편 그림은 $n=2$인 경우에 3차원 상에서 \n",
    "그려지는 함수의 그래프와 특정 지점에서의 \n",
    "그레이디언트를 보여주고 있다. \n",
    "\n",
    "* 왼편 그림\n",
    "    * 그레이디언트는 접선(tangent line)의 기울기(slope)를 가리키는 미분값 $f'(x)$이다.\n",
    "    * 갈색 직선이 접선을 가리킨다.\n",
    "* 오른편 그림\n",
    "    * 그레이디언트는 편미분값으로 구성된 길이가 2인 벡터\n",
    "        $\\left( \\frac{\\partial}{\\partial v_1} f(\\mathbf{v}), \\frac{\\partial}{\\partial v_2} f(\\mathbf{v}) \\right)$ 로 계산되며, 위쪽으로 향하는 파란색 화살표로 표시된다.\n",
    "    * 파란색 초평면(hyperplane)은 해당 지점에서 그래프와 접하는 평면을 보여준다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/Tangent-line.png\" alt=\"경사하강법\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/tangent_space-90.png\" alt=\"경사하강법\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            &#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>\n",
    "        </td>\n",
    "        <td>\n",
    "            &#60;출처\t&#62; <a href=\"\"https://github.com/pvigier/gradient-descent>pvigier: gradient-descent </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 경사하강법 작동 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법은 다음 과정을 반복하여 최솟값 지점을 찾아가는 것을 의미한다. \n",
    "\n",
    "* 해당 지점에서 그레이디언트(gradient)를 계산한다.\n",
    "* 계산된 그레이디언트의 방향(반대방향)으로 그레이디언트 크기의 일정 비율만큼 이동한다. \n",
    "\n",
    "아래 그림은 2차원 함수의 최솟값 지점을 경사하강법으로 찾아가는 과정을 보여준다.\n",
    "최솟값 지점은 해당 지점에서 구한 그레이디언트의 반대방향으로 조금씩 이동하는 방식으로 이루어진다. \n",
    "\n",
    "최솟값 지점에 가까워질 수록 그레이디언트는 점점 0벡터에 가까워진다. \n",
    "따라서 그레이디언트가 충분히 작아지면 최솟값 지점에 매우 가깞다고 판단하여 그 위치에서\n",
    "최솟값 지점의 근사치를 구하여 활용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Gradient-Descent.gif\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"\"https://github.com/pvigier/gradient-descent>pvigier: gradient-descent </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주의사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법은 지역 최솟값(local minimum)이 없고 전역 최솟값(global mininum)이 존재할 경우 유용하게 활용된다.\n",
    "반면에 지역 최솟값이 존재할 경우 제대로 작동하지 않을 수 있기 때문에 많은 주의를 요한다. \n",
    "아래 그림은 출발점과 이동 방향에 따라 도착 지점이 전역 또는 지역 최솟점이 될 수 있음을 잘 보여준다.\n",
    "하지만 이런 경우는 여기서 다루지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Gradient.png\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 파이썬으로 그레이디언트 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단변수 함수와 다변수 함수의 경우 그레이디언트 계산이 조금 다르다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단변수 함수의 도함수 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f$가 단변수 함수(1차원 함수)일 때, 점 $x$에서의 그레이디언트는 다음과 같이 구한다. \n",
    "\n",
    "$$\n",
    "f'(x) = \\lim_{h\\to 0} \\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "즉, $f'(x)$ 는 $x$가 아주 조금 변할 때 $f(x)$가 변하는 정도, 즉\n",
    "함수 $f$의 $x$에서의 **미분값**이 된다.\n",
    "이때 함수 $f'$ 을 함수 $f$의 **도함수**라 부른다.\n",
    "\n",
    "예를 들어, 제곱 함수 $f(x) = x^2$에 해당하는 `square()`가 아래와 같이 주어졌다고 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x: float) -> float:\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면 $f$의 도함수 $f'(x) = 2x$는 아래 `square_prime()` 함수로 구현된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_prime(x: float) -> float:\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 도함수 근사치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이와 같이 미분함수가 구체적으로 주어지는 경우는 일반적이지 않다.\n",
    "따라서 여기서는 충분히 작은 $h$에 대한 함수의 변화율\n",
    "\n",
    "$$\n",
    "\\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "을 측정하여\n",
    "미분값의 근사치로 사용한다.\n",
    "\n",
    "아래 그림은 $h$가 작아질 때 \n",
    "두 점 $f(x+h)$ 와 $f(x)$ 지나는 직선이 변하는 과정을 보여준다.\n",
    "$h$가 0에 수렴하면 최종적으로 점 $x$에서의 접선이 되고\n",
    "미분값 $f'(x)$는 접선의 기울기가 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Derivative.gif\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 임의의 단변수 함수에 대해 함수 변화율을 구해주는 함수를 간단하게 구현한 것이다.\n",
    "          \n",
    "* `Callable`은 함수에 대한 자료형을 가리킨다. \n",
    "    * `Callable[[float], float]`: 부동소수점을 하나 받아 부동소수점을 계산하는 함수들의 클래스를 가리킴.\n",
    "* `different_quotient()` 함수가 사용하는 세 인자와 리턴값의 자료형은 다음과 같다. \n",
    "    * 미분 대상 함수: `f: Callable[[float], float]`\n",
    "    * 미분 위치: `x: float`\n",
    "    * 인자가 변하는 정도: `h: float`\n",
    "    * 리턴값(`float`): $f'(x)$의 근사치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def difference_quotient(f: Callable[[float], float],\n",
    "                        x: float,\n",
    "                        h: float) -> float:\n",
    "    \"\"\"\n",
    "    함수 f의 x에서의 미분값 근사치 계산\n",
    "    f: 미분 대상 함수\n",
    "    x: 인자\n",
    "    h: x가 변하는 정도\n",
    "    \"\"\"\n",
    "    \n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 `square()`의 도함수 `square_prime()` 을 `difference_quotient()`를 \n",
    "이용하여 충분히 근사적으로 구현할 수 있음을 보여준다. \n",
    "근사치 계산을 위해 `h=0.001` 를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "h = 0.001\n",
    "xs = range(-10, 11)\n",
    "\n",
    "actuals = [square_prime(x) for x in xs]\n",
    "estimates = [difference_quotient(square, x, h) for x in xs]\n",
    "\n",
    "plt.title(\"Actual Derivatives vs. Estimates\")\n",
    "\n",
    "# 실제 도함수 그래프(빨간색 직선)\n",
    "plt.plot(xs, actuals, 'r-', label='square_prime') \n",
    "\n",
    "# 근사치 그래프(검은색 점)\n",
    "plt.plot(xs, estimates, 'k.', label='Estimates')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다변수 함수의 그레이디언트 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다변수 함수의 그레이디언트는 매개변수 각가에 대한 **편도함수**(partial derivative)로\n",
    "구성된 벡터로 구성된다.\n",
    "예를 들어, $i$번째 편도함수는 $i$번째 매개변수를 제외한 다른 모든 매개변수를 고정하는 \n",
    "방식으로 계산된다. \n",
    "\n",
    "$f$가 다변수 함수(다차원 함수)일 때, 점 $\\mathbf{v}$에서의 $i$번째 도함수는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial v_i}f(\\mathbf{v}) = \\lim_{h \\to 0} \\frac{f(\\mathbf{v}_{i, h}) - f(\\mathbf x)}{h}\n",
    "$$\n",
    "\n",
    "여기서 $\\mathbf{v}_{i, h}$는 $\\mathbf{v}$의 $i$번째 항목에 $h$를 더한 벡터\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_{i, h} = (v_1, \\dots, v_{i-1}, v_i + h, v_{i+1}, \\dots, v_n)\n",
    "$$\n",
    "\n",
    "를 가리킨다.\n",
    "즉, $\\frac{\\partial}{\\partial v_i} f(\\mathbf{v})$ 는 $v_i$가 아주 조금 변할 때 \n",
    "$f(\\mathbf{v})$가 변하는 정도, 즉\n",
    "함수 $f$의 $\\mathbf{v}$에서의 $i$번째 **편미분값**이 된다.\n",
    "이때 함수 $\\frac{\\partial}{\\partial v_i}f$ 를 함수 $f$의 $i$번째 **편도함수**라 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 편도함수 근사치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드에서 정의된 `partial_difference_quotient()`는 주어진 다변수 함수의 $i$번째\n",
    "편도함수의 근사치를 계산해주는 함수이다.\n",
    "사용되는 매개변수는 `difference_quotient()` 함수의 경우와 거의 같다.\n",
    "다만 $i$번째 편도함수의 근사치를 지정하기 위해 $i$번째 매개변수에만 $h$가 더해짐에 주의하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f: Callable[[LA.Vector], float],\n",
    "                                v: LA.Vector,\n",
    "                                i: int,\n",
    "                                h: float) -> float:\n",
    "    \"\"\"\n",
    "    함수 f의 v에서의 i번째 편미분값 근사치 계산\n",
    "    f: 편미분 대상 함수\n",
    "    v: 인자 벡터\n",
    "    i: i번째 인자를 가리킴\n",
    "    h: 인자 v_i가 변하는 정도\n",
    "    \"\"\"\n",
    "    \n",
    "    # v_i에 대해서만 h를 더한 벡터\n",
    "    w = [v_j + (h if j == i else 0) for j, v_j in enumerate(v)]\n",
    "\n",
    "    return (f(w) - f(v)) / h    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 `estimate_gradient()` 함수는 편미분 근사치를 이용하여 \n",
    "그레이디언트의 근사치에 해당하는 벡터\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\frac{f(\\mathbf{v}_{1, h}) - f(\\mathbf x)}{h}\\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{f(\\mathbf{v}_{n, h}) - f(\\mathbf x)}{h}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "를 리스트로 계산한다. \n",
    "근사치 계산에 사용된 $h$의 기본값은 0.0001이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f: Callable[[LA.Vector], float],\n",
    "                      v: LA.Vector,\n",
    "                      h: float = 0.0001):\n",
    "    \n",
    "    return [partial_difference_quotient(f, v, i, h) for i in range(len(v))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__참고:__ \n",
    "그레이디언트를 두 개의 함수값의 차이를 이용하여 근사치로 계산하는 방식은 계산 비용이 크다.\n",
    "벡터 `v`의 길이가 $n$이면 `estimate_gradient()` 함수를 호출할 때마다\n",
    "`f`를 $2n$ 번 호출해야 하기 때문이다. \n",
    "따라서 앞으로는 그레이디언트 함수가 수학적으로 쉽게 계산되는 경우만을 \n",
    "사용하여 경사하강법의 용도를 살펴볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 경사하강법으로 최솟점 구하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 정의한 제곱함수 `sum_of_squares()` 는 `v`가 0 벡터일 때 가장 작은 값을 갖는다. \n",
    "이 사실을 경사하강법을 이용하여 확인해보자. \n",
    "\n",
    "먼저, `sum_of_squares()` 함수의 그레이디언트는 다음과 같이 정의된다. \n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{v}) =\n",
    "\\begin{bmatrix}\n",
    "    2v_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    2v_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "아래 코드는 리스트를 이용하여 그레이디언트를 구현하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares_gradient(v: LA.Vector) -> LA.Vector:\n",
    "    return [2 * v_i for v_i in v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 임의의 지점(`v`)에서 계산된 그레이디언트에 학습률(learning rate)이라는 특정 상수를 곱한 값을\n",
    "빼서 새로운 지점을 계산하는 함수를 구현한다.\n",
    "\n",
    "__참고:__ `subtractV`, `scalar_multV`, `distance` 등은 선형대수 부분에서 정의한 \n",
    "`pydata06_linear_algebra_basics.py` 모듈에서 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def gradient_step(v: LA.Vector, gradient: LA.Vector, learning_rate: float) -> LA.Vector:\n",
    "    step = LA.scalar_multV(learning_rate, gradient)\n",
    "    new_V = LA.subtractV(v, step)                    # 그레이디언트의 반대방향으로 이동\n",
    "\n",
    "    return new_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 임의의 지점에서 출발하여 `gradient_step()` 함수를 충분히 반복하면\n",
    "제곱함수의 최솟점에 충분히 가깝게 근사할 수 있음을 아래 코드가 보여준다.\n",
    "실제로 전역 최솟값 위치인 원점에 매우 가깝게 접근한다.\n",
    "\n",
    "* `random.seed(42)`: 실행할 때마다 동일한 결과를 보장해준다. 사용하지 않으면 매번 다른 결과가 나옴.\n",
    "* `grad`: 이동할 때마다 계산된 그레이디언트. 최종적으로 0 벡터에 가까운 값을 갖게 됨.\n",
    "* `if epoch%100 == 0`: 위치 이동을 100번 할 때마다 현재 위치 확인\n",
    "* `epoch`(에포크): 여기서는 그레이디언트 계산 횟수. 즉, 이동횟수를 가리킴.\n",
    "* `learning_rate=0.01`: 그레이디언트 반대 방향, 즉, 최솟값 지점을 향해 이동할 때 사용되는 크기 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [2.732765249774521, -9.309789197635727, -4.409425359965263]\n",
      "100 [0.3624181137897112, -1.2346601088642208, -0.5847760329896551]\n",
      "200 [0.048063729299005625, -0.16374007531854073, -0.07755273779298358]\n",
      "300 [0.006374190434279768, -0.02171513607091833, -0.010285009644527733]\n",
      "400 [0.0008453423045827667, -0.002879851701919325, -0.0013639934114305214]\n",
      "500 [0.00011210892101281368, -0.00038192465375128993, -0.00018089220046728499]\n",
      "600 [1.4867835316559317e-05, -5.065067796575345e-05, -2.3989843290795995e-05]\n",
      "700 [1.971765716798422e-06, -6.717270417586384e-06, -3.1815223632100903e-06]\n",
      "800 [2.6149469369030636e-07, -8.908414196052704e-07, -4.2193208287814765e-07]\n",
      "900 [3.467931014604295e-08, -1.1814299344070243e-07, -5.5956445449048146e-08]\n",
      "\n",
      "----\n",
      "\n",
      "그레이디언트의 최종 값: [9.57758165411209e-09, -3.262822016281278e-08, -1.5453808714914104e-08]\n",
      "v의 최후 위치와 최솟점 사이의 거리: 1.830234305038648e-08\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# 임의로 선택된 출발점 좌표\n",
    "v = [random.uniform(-10, 10) for i in range(3)]\n",
    "\n",
    "# gradient_step 1000번 반복\n",
    "for epoch in range(1000):\n",
    "    grad = sum_of_squares_gradient(v)  # 그레이디언트 계산\n",
    "    \n",
    "    v = gradient_step(v, grad, 0.01)   # 좌표 업데이트\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        print(epoch, v)\n",
    "\n",
    "print(\"\\n----\\n\")        \n",
    "print(f\"그레이디언트의 최종 값: {grad}\")\n",
    "print(f\"v의 최후 위치와 최솟점 사이의 거리: {LA.distance(v, [0, 0, 0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 에포크와 학습률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 사용된 코드에서 에포크(epoch)는 이동 횟수를 가리키며, \n",
    "에포크를 크게 하면 최솟값 지점에 보다 가까워진다.\n",
    "하지만 항상 수렴하는 방향으로 이동하는 것은 아니다.\n",
    "하지만 여기서는 학습률을 너무 크게 지정하지만 않으면 항상 최솟값 지점에 수렴하는 \n",
    "볼록함수만 다룬다. \n",
    "학습률에 따른 수렴속도는 다음과 같다.\n",
    "\n",
    "* 학습률 크게: 수렴 속도가 빨라진다.\n",
    "* 학습률 작게: 수렴 속도가 느려진다.\n",
    "\n",
    "하지만 다루는 함수에 따른 적당한 학습률이 달라지며,\n",
    "보통 여러 실혐을 통해 정해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 경사하강법과 선형회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 데이터들의 분포에 대한 선형회귀 모델의 훈련과정을 구현해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 세트 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 $y = f(x) = 5 + 20x$ 일차함수의 그래프에 해당하는 데이터를 구한다. \n",
    "여기서 $x$ 는 -0.5에서 0.5 사이에 있는 100개의 값으로 주어지며,\n",
    "$y$ 값에 약간의 잡음(가우시안 잡음)이 추가된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x는 -0.5에서 0.5 사이\n",
    "X = [x/100 for x in range(-50, 50)]\n",
    "\n",
    "# 약간의 잡음 추가 (가우시안 잡음)\n",
    "error = [random.randrange(-100,100)/100 for _ in range(-50, 50)]\n",
    "\n",
    "# y = 5 + 20*x + 가우시안 잡음\n",
    "y = [5 + 20*x + e for x, e in zip(X, error)]\n",
    "\n",
    "# (x,y) 좌표값들의 리스트\n",
    "inputs = list(zip(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 프로그램은 잡음이 포함되어 직선으로 그려지지 않는 \n",
    "데이터의 분포를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUvElEQVR4nO3db4wc913H8c/31nGQ+CPSdZOaJIvzIA+ImlJgCbWC4CAtSkJEGqmgBshFKpxblUO1VInmWolW8gOHqlCDCAW7TbAFNFTQP1EUKK3ByoNeUc+AqoQAjUJyhERxco2gPLF1t18e3O5lMjezO7vzm9n5835Jkff25mZ/Ezsf//Kd7+835u4CANTXwrwHAADIhyAHgJojyAGg5ghyAKg5ghwAam7fPD70wIEDfujQoXl8NADU1vnz519x9zfG359LkB86dEjr6+vz+GgAqC0zey7pfUorAFBzBDkA1BxBDgA1R5ADQM0R5ABQcwQ5ANQcQQ4ABVlbW9Px48e1trZW6OfMpY8cAJpubW1Nt9xyiy5duqT9+/fr7NmzOnz4cCGfRZADQAHOnTunS5cuaXt7W5cuXdK5c+d2319cXAwa6pmD3MwelHSHpAvu/ubhex+TtCzp5eFhH3b3x4KNDgBqZm1tTefOnVO329X+/ft3Z+TdbrewGfo0M/I/lfSHks7E3v+ku38iyGgAoMbi5ZQTJ05oc3NTi4uLiTP00oPc3R83s0NBPhUAGige1pubm1pdXd39fnSGvri4GOxzQ9TIV8xsSdK6pA+6+6tJB5nZEUlHJKnX6wX4WAColsXFxdSwPnz4sM6ePVtIjdymefjycEb+aKRGfpWkVyS5pGOSDrr7eyadp9/vO7sfAmiiUY08dFhLkpmdd/d+/P1cM3J3fynyAackPZrnfABQd4cPHy6szTBNrgVBZnYw8uVdkp7INxwAwLSmaT/8rKRFSQfM7HlJH5W0aGZv1U5p5VlJ7w0/RADAONN0rdyd8PZnAo4FADAD9loBgJpjiT4AKFu3SfQYqZjl9rMgyAG0XpYNrqLHdDodmZm2trb2rOCcR6gT5ABaL8vy+egxg8FAkuTuunjxolZWVjQYDArf5TANNXIArTdakdnpdFKXz0ePueyyy3ZfLywsaHt7e88uh2ViRg6g9bIsn48fI2l3l8OjR48WsodKVlMt0Q+FJfoAmqTIZflRhSzRB4A6CxXA81iWH0WQA2ilMh/FVjRudgJovKSHIKc9iq2OmJEDaLS0mfe4vcPLqnmHQpADaKRRGG9sbCT2iKd1qox7XFtVQ50gB9A48VWY+/btRF3SU3vGLfypwmKfLAhyAI0TDWNJWl5eVq/XyzSrjpZczGx3JWfoByaHRJADqK20Wna8/r20tJQ5gKMllyos9smCBUEAamUU3vGQjZc9Qt2wrNKNTxYEAaitpPA2Mw0Gg9SyR6hFOvNe7JMFQQ6g0qI3LqPhvbCwsLudbJXLHmUgyAFUWvTGZTy8Z2kNrFKpJBSCHEClxW9c5unrbtKy/CiCHEClZdliNqssD5CoI4IcQGWklT1C3XActyy/zghyAJVQRtkj5Oy+SghyAJVQVtmjDu2E02IbWwCVkOW5mUjGjBxAJTS17FEGghxAZeQtezSxRzwLghxAIzS1RzwLauQAGqFJj26bFkEOoJKSnrM5TptvllJaAVA5s5RJ2nyzlCAHUDmz9pQ3sUc8C0orACqnzWWSWTAjB5DbtG1/k45vc5lkFgQ5gFyy1rPTHtGWti1tW8sksyDIAeSSpZ6d9pSfixcvamVlRYPBoHW93yFRIweQS5Z6djTsB4OBOp2OOp2OFhYWtL293cre75CYkQPIJUs9O+0pP/EyCzc1Z2Punu1Aswcl3SHpgru/efjeGyT9paRDkp6V9Evu/uqkc/X7fV9fX59xyACqLO1G5rTvYy8zO+/u/T3vTxHkPyXp/ySdiQT5xyV9293vN7P7JF3h7h+adC6CHGimNu93Uoa0IM9cI3f3xyV9O/b2nZJOD1+flvTOWQcIoF6SltC3eb+TecpbI7/K3V+UJHd/0cyuTDvQzI5IOiJJvV4v58cCmKe0mXe8Ft7tdnX8+HHKJgUr7Wanu5+UdFLaKa2U9bkAwktrOYze+IzfyKTMUpy87YcvmdlBSRr+eiH/kABU3biWw8OHD2t1dVWbm5uUWUqSd0b+iKR7Jd0//PVLuUcEoPJmaTmktbA403StfFbSoqQDkl6S9FFJX5T0OUk9SRuSftHd4zdE96BrBShP3va+PD9Pa2FYudsPQyLIgXLkbQeM/3zavigoR1qQs7ITaLBZ9/VO+nn2Raku9loBGizvvt7Rn2dflOpiRg40WN59vce1E3LzsjqokQPIjJuX80WNHEBuPOyhmqiRAw2RtPcJ2oEZOdAAIdoMKZnUF0EONECeNkN6xeuPIAcaIOty+KSZN73i9UeQAzUwqfSRpc0wy9azZrb7XM1ZFhBhPghyoOKy1r8ndZTMsvUsveL1QJADFZd3mf3IpIc+jM554403cuOzZghyoOJCbQeb9aEP9IrXD33kQMWNAvjYsWOZbz6m9ZTz0IdmYkYO1MA0s+QsNXUe+tAsBDnQMFlq6nk300K1EORAhYRYYZl1tk0tvDkIcqAi8i6zH2G23T4EOVARodoMJWbbbUPXClCytI6SvE/zQXsxIwdKNK58QkkEsyLIgRKMbmJubGyMLZ9QEsEsCHKgYNFZeKfT0b59O//ZhSifsI84JIIcKFz0JqYkLS8vq9fr7YbvtGE8On7cMnu0C0EOFCze1720tLQbuONq5kkBHz3ezDQYDNhyFgQ5ULRxNzHTWg7TAj56/MLCgjqdjsyMLpeWI8iBgNLKJGk3MdNWYaYFfPx4HssGiSAHgpllZWbabD0t4GlRRBKCHAhk1pWZSbP1cYFNiyLiCHIgkNBbwxLYyIogBwKh7IF5IciBgKKzaBbroCwEOVCAUFvSAlmw+yGg9B0JZ5V04xMoCjNytN60qyvHvT/CMzFRJoIcrTft6sosZRNufKJMBDlab9rVlVn7xWkfRFkIcrTetKsrKZugaszd85/E7FlJ35G0LWnL3fvjju/3+76+vp77c4EiROvfkmaqkQNFMLPzSfkaMsj77v5KluMJclQVbYOosrQgp/0QiJilbTB06yIwrVA1cpf0d2bmkv7E3U8GOi9Qqmnr38zgUQWhgvxmd3/BzK6U9BUz+zd3fzx6gJkdkXREknq9XqCPBSabpp49rm0w6Tyz7ngIhBQkyN39heGvF8zsC5JukvR47JiTkk5KOzXyEJ8LTDLrHuHxY9LOQwcLqiB3jdzMvtvMvnf0WtLPSXoi73mBEEItlU87z2gGf+zYMcoqmJsQM/KrJH3BzEbn+wt3/9sA5wVyCzVjHnceFv5g3oK0H06L9kOUKVTPN73jmLdC+8inRZADwPToIwdEzzeaib1W0Br0fKOpmJGjkZJm3jzsAU3FjByNQ8832oYZORohOgPP2vMtiXo5GoEZOeYqREtffAZ+4sSJiT3f1MvRJAQ55iZUmMZn4JubmxMfs8YeKWgSghxzM0uYJs3gk2rfk1ZbUi9HkxDkmJtQW8bO8qBjHo6MJiHIMTfThum4Gfws+52wRwqagiDHXE0TppRDgGQEOWqDcgiQjCBHrURn8OxGCOwgyFFL9IEDr2FlJypjmp0J2TcFeA0zclTCtDNsbnwCr2FGjkoYN8NOmqnzrEzgNczIUQlpM+xxM3X6wIEdBDlmErpjJN5aKO3sTLixscGeKMAEBDmmVlTHSNLOhJ1OR/v27fwxpRYOJCPIMbWidw6Mnl+SlpeX1ev16BcHUhDkSJVWPsnbMTKpLBM//9LSEgEOjEGQI9Gkm4yzLpXPUpZhKT4wHYIciSaVT9KWyo9+Nv56dGzWsgwdKUB2BDkSZS2fxG9Mmpm2trZe93r0+LXNzU11u10W8gCBEeRIlLW8EZ1hDwYDSZK7v+71xYsXtbKyosFg8LpQp2wChEGQI1WW8kZ05p42Izez3aAfPVNzdXW1pKsAmo8gRy5JC3nir7vdro4ePUo5BSiIuXvpH9rv9319fb30z8X8sHc4kJ+ZnXf3fvx9ZuQoBV0oQHHY/RAAao4gb6lpHuIAoNoorbQQj0kDmoUZeQvN8pg0ZvBAdTEjb6FpN71iBg9UG0HeQtNuSlX0trUA8iHIWyTey5206VWWbWVZ0ANUC0FeY9Msskkrj7CtLFB/QYLczG6V9PuSOpI+7e73hzgv0k1bt04rj7CtLFB/ubtWzKwj6QFJt0m6QdLdZnZD3vNivHgAnzlzZmxXyag80ul0XlceSXsfQH2EmJHfJOlpd39GkszsYUl3SvrXAOdGzKicEt3Xu9Pp6KGHHtqz93e0DJJWHqFsAtRf7k2zzOxdkm51918ffn2PpJ9w95XYcUckHZGkXq/3Y88991yuz22TaHhHdxEcBfbGxoZOnTql7e1tLSwsqNPp7O79PW2rIJtbAdVV5KZZlvDenr8d3P2kpJPSzu6HAT63FaK1cDPTYDDYs6/32tqaTp8+vXtMdO/vaVoF6RcH6inEys7nJV0b+foaSS8EOC+09wk8nU5nTz17VB45duyYHnjgAV1++eW7x3S73cwrMmdZ8Qlg/kLMyL8h6Xozu07Sf0t6t6RfDnBeaG8Pd9pj0qJdJTfeeGNiKWbSDJt+caCecge5u2+Z2YqkL2un/fBBd38y98ggababkaNQP378+FQrMrnxCdQTTwhqMGreQLPwhKAWYoYNtANBXiFFtP6xIhNoPoK8IiiDAJgVD5aoCFr/AMyKIC9Z2pN22PMEwKworZRoXPmEG5MAZkWQl2jSlrHcmAQwC0orJaJ8AqAIzMhLEG0rzFo+YRdCAFkR5AVLqouvrq5O/TOEOYA0lFYKNktbIa2IAKZBkBdslro4tXQA06C0UrBZdy+kFRFAVux+CAA1kbb7IaUVAKg5ghwAao4gn6O0fVcAYBrc7CzIpAU99IoDCIUgL0CWkJ607woAZEWQBzSahW9sbEwMaZ5YDyAUgjxm1j1OorPwTqejfft2/tWmhTS94gBCIcgj8tSto6USSVpeXlav1xsb0mxbCyCEVgX5pNl2vG595syZzDPmeKlkaWmJkAZQitYEeZbZdjSMO52OHnroIW1tbWWanVMqATAvrQnyLF0i0TDe2NjQqVOnJnaVxGf5BDiAsjU6yKMhGy99dLtdHT9+fM/seRTGa2trOn369NiuEnrBAVRBY4M8KWRHs+1ut6ujR4+ODeAspRJ6wQFUQSOW6CctdU8L2dXVVW1ubmZ6cMPo+LRwZt9wAFVQ+xl5Wnlj3IKbvItxZnkGJwAUpfZBnjbzHlcamaXDZBTeSWWZSc/gBIAi1T7Ix82ux3WRTNNhEp31m5kGg4EGgwF1cQCVUPsgL6N/OzrrX1hYUKfTkZlRFwdQCbUPcqn4pe7xWf+JEye0ublJXRxAJTQiyIvGqk0AVUaQK9uOh6zaBFBVrQ9yVmcCqLtGLAjKI6l9EQDqpPVBzupMAHWXq7RiZh+TtCzp5eFbH3b3x/IOqkzcyARQdyFq5J90908EOM/ccCMTQJ21vrQCAHUXIshXzOybZvagmV0R4Hy5JO2ECABNNrG0YmZflfSmhG99RNKnJB2T5MNff1fSe1LOc0TSEUnq9XozDnc8WgkBtNHEIHf3t2c5kZmdkvTomPOclHRSkvr9vmcd4DR40AOANsrbtXLQ3V8cfnmXpCfyDylddAWmpD2dJnn3GQeAOsrbtfJxM3urdkorz0p6b94BpYmWTUa7D8afcE8rIYA2yhXk7n5PqIFMEi2bDAaD0efvKaHQSgigbWqz10q0bBKfkVNCAdBmtQnyeNlE2lsjB4A2MvdCGkjG6vf7vr6+XvrnAkCdmdl5d+/H32dlJwDUHEEOADVHkANAzRHkAFBzBDkA1BxBDgA1N5f2QzN7WdJzpX9wfgckvTLvQZSsjdcstfO623jNUr2u+wfd/Y3xN+cS5HVlZutJPZxN1sZrltp53W28ZqkZ101pBQBqjiAHgJojyKdzct4DmIM2XrPUzutu4zVLDbhuauQAUHPMyAGg5ghyAKg5gnwMM3uDmX3FzL41/PWKMcd2zOyfzSz1AdR1kOWazexaM/sHM3vKzJ40sw/MY6whmNmtZvbvZva0md2X8H0zsz8Yfv+bZvaj8xhnSBmu+VeG1/pNM/uamf3wPMYZ0qRrjhz342a2bWbvKnN8eRHk490n6ay7Xy/p7PDrNB+Q9FQpoypWlmvekvRBd/8hSW+T9BtmdkOJYwzCzDqSHpB0m6QbJN2dcB23Sbp++M8RSZ8qdZCBZbzm/5T00+7+FknHVPObgRmveXTc70j6crkjzI8gH+9OSaeHr09LemfSQWZ2jaSfl/TpcoZVqInX7O4vuvs/DV9/Rzt/gV1d1gADuknS0+7+jLtfkvSwdq4/6k5JZ3zH1yV9v5kdLHugAU28Znf/mru/Ovzy65KuKXmMoWX5fZak35T015IulDm4EAjy8a5y9xelnfCSdGXKcSck/ZakQUnjKlLWa5YkmdkhST8i6R+LH1pwV0v6r8jXz2vvX0hZjqmTaa/n1yT9TaEjKt7EazazqyXdJemPSxxXMLV5ZmdRzOyrkt6U8K2PZPz5OyRdcPfzZrYYcGiFyXvNkfN8j3ZmMEfd/X9DjK1klvBevB83yzF1kvl6zOxntBPkP1noiIqX5ZpPSPqQu2+bJR1eba0Pcnd/e9r3zOwlMzvo7i8O/3c66X+5bpb0C2Z2u6TvkvR9ZvZn7v6rBQ05twDXLDO7TDsh/ufu/vmChlq05yVdG/n6GkkvzHBMnWS6HjN7i3ZKhbe5+2ZJYytKlmvuS3p4GOIHJN1uZlvu/sVSRpgTpZXxHpF07/D1vZK+FD/A3Vfd/Rp3PyTp3ZL+vsohnsHEa7adP+2fkfSUu/9eiWML7RuSrjez68xsv3Z+/x6JHfOIpKVh98rbJP3PqPRUUxOv2cx6kj4v6R53/485jDG0idfs7te5+6Hhf8d/Jen9dQlxiSCf5H5J7zCzb0l6x/BrmdkPmNljcx1ZcbJc882S7pH0s2b2L8N/bp/PcGfn7luSVrTTpfCUpM+5+5Nm9j4ze9/wsMckPSPpaUmnJL1/LoMNJOM1/7akrqQ/Gv7ers9puEFkvOZaY4k+ANQcM3IAqDmCHABqjiAHgJojyAGg5ghyAKg5ghwAao4gB4Ca+39uYEkbovowwQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X, y, 'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 위 그래프를 선형적으로 묘사하는 함수를 경사하강법을 이용하여 구현한다.\n",
    "즉 아래 1차 함수의 직선 그래프가 위 데이터의 분포를 최적으로 묘사하는 \n",
    "$\\theta_0$와 $\\theta_1$을 구해야 한다.\n",
    "\n",
    "$$\n",
    "\\hat y = \\theta_0 + \\theta_1\\cdot x\n",
    "$$\n",
    "\n",
    "* $\\theta_0$: 절편\n",
    "* $\\theta_1$: 기울기\n",
    "* $\\hat y$: $y$ 에 대한 예측값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비용함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법을 적용하려면 최적의 기준을 지정해야 한다. \n",
    "여기서는 예측치 $\\hat y$와 실제 $y$ 사이의 **평균제곱오차**(mean squared error, MSE)를\n",
    "최소로 하는 것을 최적의 기준으로 사용한다($m$은 $X$의 원소 개수).\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\textrm{MSE}(\\theta_0, \\theta_1) \n",
    "&= \\frac{1}{m} \\sum_{x \\in X} (\\hat y - y)^2\\\\[1ex] \n",
    "&= \\frac{1}{100}\\sum_{x \\in X} (\\theta_0 + \\theta_1\\cdot x - y)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "즉, MSE를 최소로 하는 $\\theta_0, \\theta_1$을 구해야 한다.\n",
    "($x, y$는 모두 상수임에 주의하라.)\n",
    "이처럼 최적의 기준으로 최솟값 지점을 찾는 대상인 함수를 __비용함수__라 부른다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 비용함수의 그레이언트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE의 그레이디언트는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\nabla \\textrm{MSE}(\\theta_0, \\theta_1)\n",
    "= \n",
    "\\frac{1}{100} \\cdot\n",
    "\\begin{bmatrix}\n",
    "\\sum_{x \\in X} 2(\\hat y - y) \\\\[1ex]\n",
    "\\sum_{x \\in X} 2(\\hat y - y) x\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "이처럼 전체 데이터셋을 대상으로 평균제곱오차의 그레이디언트를 활용하는 경사하강법을\n",
    "__배치 경사하강법__이라 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드의 `linear_gradient()` 함수는 특정 데이터 샘플 $x$에 대해 타깃(실제값) $y$와 예측치 $\\hat y$ 사이의 \n",
    "MSE 함수의 그레이디언트를 계산한다.\n",
    "\n",
    "| 변수 | 의미 |\n",
    "| :--- | :--- |\n",
    "| intercept | $\\theta_0$ (절편) |\n",
    "| slope | $\\theta_1$ (기울기) |\n",
    "| predicted | $\\hat y$ (예측값) |\n",
    "| error | $\\hat y - y$ (오차) |\n",
    "| grad | $x$에 대한 제곱오차의 그레이디언트 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_gradient(x: float, y: float, theta: LA.Vector) -> LA.Vector:\n",
    "    \n",
    "    intercept, slope = theta           \n",
    "    predicted = intercept + slope * x  \n",
    "    error = (predicted - y)            \n",
    "    grad = [2 * error, 2 * error * x]  \n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평균제곱오차는 이제 아래와 같이 지정된다. \n",
    "\n",
    "```python\n",
    "LA.vector_mean([linear_gradient(x, y, theta) for x, y in list(zip(X, y)])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE() 함수의 최솟값 지점 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 임의의 $\\theta = (\\theta_0, \\theta_1)$로 시작한 후에\n",
    "경사하강법으로 평균제곱오차의 최솟값 지점을 구할 수 있다.\n",
    "\n",
    "* `vector_mean`: 벡터 항목들의 평균값 계산\n",
    "* `epoch`(에포크): 5000회 반복. MSE를 5000번 계산.\n",
    "* `learning_rate`: 학습률\n",
    "\n",
    "아래 코드에서 사용한 학습률은 0.001이다.\n",
    "\n",
    "__참고:__ __에포크__는 훈련 세트에 포함된 전체 훈련 샘플에 대해 예측값을 계산하는 것을 의미한다.\n",
    "따라서 아래 코드는 MSE를 총 5,000번 계산할 때마다 $\\theta_0, \\theta_1$을 업데이트 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.5498750271006548, -0.348065046729531]\n",
      "500 [2.8916275137318697, 1.2779917858845682]\n",
      "1000 [4.161340996373953, 2.78438692444515]\n",
      "1500 [4.63252303459113, 4.174144289893471]\n",
      "2000 [4.809879153546342, 5.454184661194932]\n",
      "2500 [4.878918399660604, 6.6323964362145]\n",
      "3000 [4.907842395609395, 7.716596269085061]\n",
      "3500 [4.921739999085596, 8.714181270348421]\n",
      "4000 [4.929854101282013, 9.632032625512535]\n",
      "4500 [4.935602366135786, 10.476509096849366]\n",
      "최종 기울기: 11.252\n",
      "최종 절편: 4.940\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(5000):\n",
    "    # 평균 제곱 오차 계산\n",
    "    grad = LA.vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "    \n",
    "    # theta 값 업데이트.\n",
    "    theta = gradient_step(theta, grad, learning_rate)\n",
    "    \n",
    "    # 500번에 한 번 학습과정 확인\n",
    "    if epoch % 500 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 최종 기울기가 11.xx 정도로 애초에 사용한 20과 차이가 크다.\n",
    "이유는 학습률이 너무 낮아서 5000번의 에포크가 충북한 학습을 위해 부족했기 때문이다.\n",
    "이에 대한 해결책은 보통 두 가지이다. \n",
    "\n",
    "* 첫째: 학습률 키우기\n",
    "* 둘째: 에포크 키우기\n",
    "\n",
    "아래 코드는 먼저 에포크를 네 배 늘린 결과를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.5453216196333258, -0.8846541803415932]\n",
      "1000 [4.3071961393797, 2.3344318287971526]\n",
      "2000 [4.827835026420881, 5.073878601499392]\n",
      "3000 [4.908792115820234, 7.394749506518404]\n",
      "4000 [4.928733502968913, 9.359603724571762]\n",
      "5000 [4.939051515502755, 11.022864887660349]\n",
      "6000 [4.9468993258866565, 12.430800212519703]\n",
      "7000 [4.953422706517003, 13.62260111714172]\n",
      "8000 [4.958928503638361, 14.631446280141436]\n",
      "9000 [4.96358691111091, 15.485421540772457]\n",
      "10000 [4.967529901914979, 16.208301288794544]\n",
      "11000 [4.97086755619471, 16.82021026697407]\n",
      "12000 [4.973692834861904, 17.338183828260746]\n",
      "13000 [4.976084398412164, 17.776642193387854]\n",
      "14000 [4.97810882798133, 18.147791905104715]\n",
      "15000 [4.979822483153499, 18.46196565445812]\n",
      "16000 [4.9812730715626445, 18.727909939652793]\n",
      "17000 [4.982500977134955, 18.95302856580554]\n",
      "18000 [4.98354038437498, 19.14358876454719]\n",
      "19000 [4.98442023005287, 19.30489567177664]\n",
      "최종 기울기: 19.441\n",
      "최종 절편: 4.985\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(20000):\n",
    "    # 평균 제곱 오차 계산 (전체 훈련 데이터 대상)\n",
    "    grad = LA.vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "    \n",
    "    # theta 값 업데이트. 그레이디언트 반대 방향으로 지정된 학습률 만큼 이동\n",
    "    theta = gradient_step(theta, grad, learning_rate)\n",
    "    \n",
    "    # 1000번에 한 번 학습과정 확인\n",
    "    if epoch % 1000 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반면에 아래 코드는 학습률을 0.01로 키웠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.7285600934134986, 0.6423040193614589]\n",
      "500 [4.942726752245414, 11.692237628170421]\n",
      "1000 [4.9691293209021055, 16.501531174187775]\n",
      "1500 [4.980523087447129, 18.590411261023977]\n",
      "2000 [4.985471879329149, 19.497700269800106]\n",
      "2500 [4.987621349042239, 19.891774276545842]\n",
      "3000 [4.988554954689417, 20.062937292178695]\n",
      "3500 [4.988960459125764, 20.137280632310304]\n",
      "4000 [4.989136586860627, 20.16957109062855]\n",
      "4500 [4.989213086588395, 20.183596202986426]\n",
      "최종 기울기: 20.190\n",
      "최종 절편: 4.989\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(5000):\n",
    "    # 평균 제곱 오차 계산 (전체 훈련 데이터 대상)\n",
    "    grad = LA.vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "    \n",
    "    # theta 값 업데이트. 그레이디언트 반대 방향으로 지정된 학습률 만큼 이동\n",
    "    theta = gradient_step(theta, grad, learning_rate)\n",
    "    \n",
    "    # 500번에 한 번 학습과정 확인\n",
    "    if epoch % 500 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과를 비교했을 때 학습률을 키우는 것이 보다 효과적이다. \n",
    "최종적으로 구해진 기울기와 절편을 이용하여 처음에 주어진 데이터의 분포를\n",
    "선형적으로 학습한 1차함수의 그래프는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 예측치\n",
    "y_hat = [intercept + slope * x for x in X]\n",
    "\n",
    "# 실제 데이터 분포\n",
    "plt.plot(X, y, 'k.', label='Actuals')\n",
    "# 예측치 그래프\n",
    "plt.plot(X, y_hat, 'r-', label='Estimates')\n",
    "\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 배치/미니배치/확률적 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 사용한 배치 경사하강법은 주어진 데이터셋 전체를 대상으로 평균제곱오차의 그레이디언트를 계산하였다.\n",
    "이런 방식을 **배치 경사하강법**이라 부른다.\n",
    "\n",
    "**주의**: 배치(batch)는 원래 하나의 묶음을 나타내지만 여기서는 주어진 (훈련) 데이터셋 전체를 가리키는\n",
    "의미로 사용된다.\n",
    "\n",
    "그런데 사용된 데이터셋의 크기가 100이었기 때문에 계산이 별로 오래 걸리지 않았지만,\n",
    "데이터셋이 커지면 그러한 계산이 매우 오래 걸릴 수 있다.\n",
    "실전에서 사용되는 데이터셋의 크기는 몇 만에서 수십억까지 다양하며, \n",
    "그런 경우에 적절한 학습률을 찾는 과정이 매우 오래 걸릴 수 있다.\n",
    "\n",
    "데이터셋이 매우 큰 경우에는 따라서 아래 두 가지 방식 중 하나를 사용할 것을 추천한다.\n",
    "\n",
    "* 미니배치 경사하강법\n",
    "* 확률적 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미니배치 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니매치 경사하강법(mini-batch gradient descent)은\n",
    "전체 훈련 세트를 쪼갠 여러 개의 미니배치(작은 훈련 세트)에 대해 평균제곱오차의 그레이디언트를\n",
    "계산하여 $\\theta_0, \\theta_1, \\dots $ 를 업데이트한다. \n",
    "\n",
    "예를 들어, 전체 데이터셋의 크기가 1000이고 미니배치의 크기를 10이라 하면,\n",
    "배치 경사하강법에서는 하나의 에포크를 돌 때마다 한 번 MSE와 그레이디언트를 계산하였지만\n",
    "미니배치 경사하강법에서는 10개의 데이터를 확인할 때마다 MSE와 그레이디언트를 계산한다.\n",
    "즉,하나의 에포크를 돌 때마다 총 100번 기울기와 절편을 업데이트한다. \n",
    "\n",
    "아래 코드에서 정의된 `minibatches()` 함수는 호출될 때마다\n",
    "`batch_size`로 지정된 크기의 데이터 세트를 전체 데이터셋에서\n",
    "선택해서 내준다.\n",
    "\n",
    "데이터를 선택하는 방식은 다음과 같다.\n",
    "\n",
    "* `minibatches()`: 호출될 때마다 `batch_size` 크기 만큼의 훈련 데이터를 생성해서 제공하는 제너레이터이다.\n",
    "* `batch_starts`: 전체 데이터셋에서 차례대로 선택할 인덱스로 이루어진 어레이.\n",
    "* 섞기(`shuffle`) 옵션: `batch_starts`에 포함된 항목들의 순서를 무작위로 섞을 때 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Iterator\n",
    "\n",
    "# 제너레이터 함수 정의\n",
    "def minibatches(dataset: List[float],\n",
    "                batch_size: int,\n",
    "                shuffle: bool = True) -> Iterator[List[float]]:\n",
    "    \"\"\"\n",
    "    dataset: 전체 데이터셋\n",
    "    batch_size: 미니배치 크기\n",
    "    shuffle: 섞기 옵션\n",
    "    리턴값: 이터레이터\n",
    "    \"\"\"\n",
    "\n",
    "    # 0번 인덱스부터 시작하여, batch_size 배수 번째에 해당하는 인덱스만 선택\n",
    "    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
    "    \n",
    "    # shuffle 옵션이 참이면 인덱스 섞기\n",
    "    if shuffle: random.shuffle(batch_starts)\n",
    "\n",
    "    # batch_starts에  포함된 인덱스를 기준으로 해서 미니배치 크기만큼씩 선택해서 \n",
    "    # 다음 MSE와 그레이디언트 계산에 필요한 훈련 데이터 세트를 지정함.\n",
    "    for start in batch_starts:\n",
    "        end = start + batch_size\n",
    "        yield dataset[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 미니배치 경사하강법을 이전에 사용했던 데이터에 대해 적용한다.\n",
    "학습률(`learning_rate`)을 0.001로 하면 학습이 제대로 이루어지지 않는다.\n",
    "에포크 수를 키우거나 합습률을 크게 해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.14700601161032584, -0.850324957636322]\n",
      "100 [3.0383003005334195, 0.81879593562066]\n",
      "200 [4.21458258079334, 2.363808765349366]\n",
      "300 [4.65037766647808, 3.7887387104243513]\n",
      "400 [4.814916905651929, 5.100937311842601]\n",
      "500 [4.880054707558669, 6.308581062740375]\n",
      "600 [4.906941797916403, 7.419765388918784]\n",
      "700 [4.921300749140654, 8.442074919728974]\n",
      "800 [4.928451522266928, 9.382601070352939]\n",
      "900 [4.934261548296817, 10.247863490712117]\n",
      "최종 기울기: 11.036\n",
      "최종 절편: 4.939\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률 지정\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 1000번의 에포크\n",
    "for epoch in range(1000):\n",
    "    # 미니배치의 크기를 20으로 지정함\n",
    "    # 따라서 한 번의 에포크마다 5번 MSE와 그레이디언트 계산 후 기울기와 절편 업데이트\n",
    "    \n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = LA.vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, learning_rate)\n",
    "\n",
    "    # 100개의 에포크가 지날 때마다 학습 내용 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률을 0.01로 키우면 좋은 결과가 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1.237825277922001, 0.7249192271332124]\n",
      "100 [4.952368308578804, 11.798388090708]\n",
      "200 [4.970608256534244, 16.578080917985734]\n",
      "300 [4.983392717759511, 18.637033407981054]\n",
      "400 [4.986056813379833, 19.523825735584676]\n",
      "500 [4.987595092175629, 19.90597277649338]\n",
      "600 [4.988266095521663, 20.070316968833907]\n",
      "700 [4.989040195003638, 20.14103697686222]\n",
      "800 [4.989672228772879, 20.17161547560229]\n",
      "900 [4.988703699118148, 20.184765698131514]\n",
      "최종 기울기: 20.190\n",
      "최종 절편: 4.989\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률 지정\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 1000번의 에포크\n",
    "for epoch in range(1000):\n",
    "    # 미니배치의 크기를 20으로 지정함\n",
    "    # 따라서 한 번의 에포크마다 5번 MSE와 그레이디언트 계산 후 기울기와 절편 업데이트\n",
    "\n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = LA.vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, learning_rate)\n",
    "    \n",
    "    # 100개의 에포크가 지날 때마다 학습 내용 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률을 0.01로 두고 에포크를 3000으로 늘려도 성능이 그렇게 좋아지지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.43549987222961334, 0.26254473969499653]\n",
      "300 [4.978902536543744, 18.599116202912136]\n",
      "600 [4.988010821486811, 20.067010548132913]\n",
      "900 [4.989101560704833, 20.18451199354063]\n",
      "1200 [4.989313445216165, 20.193984886037637]\n",
      "1500 [4.98927937941379, 20.195015565259936]\n",
      "1800 [4.988570103335046, 20.195105271575724]\n",
      "2100 [4.989706094445955, 20.194956727051277]\n",
      "2400 [4.990361301352804, 20.194971623699445]\n",
      "2700 [4.989683544148361, 20.19504893855309]\n",
      "최종 기울기: 20.194\n",
      "최종 절편: 4.989\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률 지정\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 1000번의 에포크\n",
    "for epoch in range(3000):\n",
    "    # 미니배치의 크기를 20으로 지정함\n",
    "    # 따라서 한 번의 에포크마다 5번 MSE와 그레이디언트 계산 후 기울기와 절편 업데이트\n",
    "    \n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = LA.vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, learning_rate)\n",
    "    \n",
    "    # 100개의 에포크가 지날 때마다 학습 내용 출력\n",
    "    if epoch % 300 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 과정을 살펴보면 기울기가 20.195 정도에서 더 이상 좋아지지 않는다.\n",
    "이런 경우 특별히 더 좋은 결과를 얻을 수 없다는 것을 의미한다.\n",
    "사실, 데이터셋을 지정할 때 가우시안 잡음을 추가하였기에 완벽한 선형함수를 찾는 것은 애초부터 불가능하다.\n",
    "따라서 위 결과를 미니배치 경사하강법을 사용한 선형회귀로 얻을 수 있는 최선으로 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 확률적 경사하강법(SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률적 경사하강법(stochastic gradient descent, SGD)은\n",
    "미니배치의 크기가 1인 미니배치 경사하강법을 가리킨다.\n",
    "즉, 하나의 데이터를 학습할 때마다 그레이디언트를 계산하여 기울기와 절편을 업데이트 한다. \n",
    "\n",
    "아래 코드는 주어진 데이터셋을 대상로 SGD를 적용하는 방식을 보여준다.\n",
    "학습률을 0.001로 했음에도 불구하여 1000번의 에포크를 반복한 후에 이전에\n",
    "0.01을 사용했던 경우와 거의 동일한 결과를 얻는다는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.550382182232026, 0.9895536437047407]\n",
      "100 [5.032490886155949, 16.554658325069674]\n",
      "200 [4.997519092010245, 19.51236265820966]\n",
      "300 [4.990918695543546, 20.070584404084553]\n",
      "400 [4.989672971014022, 20.175940275546683]\n",
      "500 [4.9894378594398585, 20.19582459523417]\n",
      "600 [4.9893934857031255, 20.19957745840122]\n",
      "700 [4.989385110834649, 20.20028575429328]\n",
      "800 [4.989383530205495, 20.200419434379114]\n",
      "900 [4.989383231885758, 20.20044466446389]\n",
      "최종 기울기: 20.200\n",
      "최종 절편: 4.989\n"
     ]
    }
   ],
   "source": [
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 에포크는 1000\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    # 매 훈련 샘플에 대해 예측값을 확인한 후 바로 theta 값 업데이트\n",
    "    for x, y in inputs:\n",
    "        grad = linear_gradient(x, y, theta)\n",
    "        theta = gradient_step(theta, grad, learning_rate)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률을 0.01로 하면 결과가 오히려 좀 더 나빠진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [6.8116974087303825, 2.3020744483619793]\n",
      "100 [4.990193170861476, 20.26103005086055]\n",
      "200 [4.990192825899716, 20.26103222379059]\n",
      "300 [4.990192825899663, 20.26103222379092]\n",
      "400 [4.990192825899663, 20.26103222379092]\n",
      "500 [4.990192825899663, 20.26103222379092]\n",
      "600 [4.990192825899663, 20.26103222379092]\n",
      "700 [4.990192825899663, 20.26103222379092]\n",
      "800 [4.990192825899663, 20.26103222379092]\n",
      "900 [4.990192825899663, 20.26103222379092]\n",
      "최종 기울기: 20.261\n",
      "최종 절편: 4.990\n"
     ]
    }
   ],
   "source": [
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 에포크는 1000\n",
    "for epoch in range(1000):\n",
    "    for x, y in inputs:\n",
    "        grad = linear_gradient(x, y, theta)\n",
    "        theta = gradient_step(theta, grad, learning_rate)\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 살펴본 것에 따르면 확률적 경사하강법의 성능이 가장 좋았다.\n",
    "하지만 이것은 경우에 따라 다르다.\n",
    "아래 그림은 각각의 방식에서 절편과 기울기가 학습되는 과정을 잘 보여준다.\n",
    "\n",
    "- 배치학습: 수렴값에 진동 없이 가장 빠르게 접근.\n",
    "- 미니배치 학습: 중간 속도로 접근함. 수렴값 근처에서 진동이 어느 정도 있음.\n",
    "- 확률적 경사 하강법: 매우 변화가 심함. 극한값 근처에서 여전히 진동이 심함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml2/master/slides/images/ch04/homl04-05.png\" width=\"500\"/></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
