{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 머신러닝 맛보기 3편"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주요 내용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 경사하강법 의미\n",
    "1. 그레이디언트 계산\n",
    "1. 경사하강법과 선형회귀\n",
    "1. 미니배치/확률적 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필수 모듈 불러오기\n",
    "\n",
    "선형대수에서 정의한 함수를 사용하기 위한 준비가 필요하며\n",
    "필요한 코드가 `pydata06_linear_algebra_basics.py` 파일에 저장되어 있다고 가정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydata06_linear_algebra_basics as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 1: 경사하강법 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 데이터셋을 가장 잘 반영하는 최적의 수학적 모델을 찾으려 할 때 가장 기본적으로 사용되는 기법이\n",
    "**경사하강법**(gradient descent)이다.\n",
    "최적의 모델에 대한 기준은 학습법에 따라 다르지만, \n",
    "보통 학습된 모델의 오류를 최소화하도록 유도하는 기준을 사용한다. \n",
    "\n",
    "여기서는 선형회귀 모델을 학습하는 데에 기본적으로 사용되는 **평균 제곱 오차**(mean squared error, MSE)를\n",
    "최소화하기 위해 경사하강법을 적용하는 과정을 살펴본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 기본 아이디어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 $f:\\textbf{R}^n \\to \\textbf{R}$의 최댓값(최솟값)을 구하고자 한다.\n",
    "예를 들어, \n",
    "길이가 $n$인 실수 벡터를 입력받아 항목들의 제곱의 합을 계산하는 함수가 다음과 같다고 하자.\n",
    "\n",
    "$$\n",
    "f(\\mathbf x) = f(x_1, ..., x_n) = \\sum_{k=1}^{n} x_k^2 = x_1^2 + \\cdots x_n^2\n",
    "$$\n",
    "\n",
    "아래 코드에서 정의된 `sum_of_squares()`가 위 함수를 파이썬으로 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(v: LA.Vector) -> float:\n",
    "    \"\"\"\n",
    "    v 벡터에 포함된 원소들의 제곱의 합 계산\n",
    "    \"\"\"\n",
    "    return LA.dot(v, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 `sum_of_squares(v)`가 최대 또는 최소가 되는 벡터 `v`를 찾고자 한다.\n",
    "\n",
    "그런데 특정 함수의 최댓값(최솟값)이 존재한다는 것이 알려졌다 하더라도 실제로 최댓값(최솟값)\n",
    "지점을 확인하는 일은 일반적으로 매우 어렵고, 경우에 따라 불가능하다. \n",
    "따라서 보통 해당 함수의 그래프 위에 존재하는 임의의 점에서 시작한 후\n",
    "그레이디언트를 방향(반대 방향)으로 조금씩 이동하면서 최댓값(최솟값) 지점을 찾아가는\n",
    "**경사하강법**(gradient descent)을 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그레이디언트의 정의와 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 $f:\\textbf{R}^n \\to \\textbf{R}$가 vector $\\textbf{x}\\in \\textbf{R}$에서 \n",
    "미분 가능할 때 그레이디언트는 다음처럼 편미분으로 이루어진 벡터로 정의된다. \n",
    "\n",
    "$$\n",
    "\\nabla f(\\textbf{x}) =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial x_1} f(\\textbf{x}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial}{\\partial x_n} f(\\textbf{x})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "아래에서 왼편 그림은 $n=1$인 경우 2차원 상에서, 오른편 그림은 $n=2$인 경우에 3차원 상에서 \n",
    "그려지는 함수의 그래프와 특정 지점에서의 \n",
    "그레이디언트를 보여주고 있다. \n",
    "\n",
    "* 왼편 그림\n",
    "    * 그레이디언트는 접선(tangent line)의 기울기(slope)를 가리키는 미분값 $f'(x)$이다.\n",
    "    * 갈색 직선이 접선을 가리킨다.\n",
    "* 오른편 그림\n",
    "    * 그레이디언트는 편미분값으로 구성된 길이가 2인 벡터\n",
    "        $(\\frac{\\partial}{\\partial x_1} f(\\textbf{x}), \\frac{\\partial}{\\partial x_2} f(\\textbf{x}))$ 로 계산되며, 위쪽으로 향하는 파란색 화살표로 표시된다.\n",
    "    * 파란색 초평면(hyperplane)은 해당 지점에서 그래프와 접하는 평면을 보여준다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/Tangent-line.png\" alt=\"경사하강법\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/tangent_space-90.png\" alt=\"경사하강법\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            &#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>\n",
    "        </td>\n",
    "        <td>\n",
    "            &#60;출처\t&#62; <a href=\"\"https://github.com/pvigier/gradient-descent>pvigier: gradient-descent </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 경사하강법 작동 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법은 다음 과정을 반복하여 최댓값(최솟값) 지점을 찾아가는 것을 의미한다. \n",
    "\n",
    "* 해당 지점에서 그레이디언트(gradient)를 계산한다.\n",
    "* 계산된 그레이디언트의 방향(반대방향)으로 그레이디언트 크기의 일정 비율만큼 이동한다. \n",
    "\n",
    "아래 그림은 2차원 함수의 최솟값을 경사하강법으로 찾아가는 과정을 보여준다.\n",
    "최솟값은 해당 지점에서 구한 그레이디언트의 반대방향으로 조금씩 이동하는 방식으로 이루어진다. \n",
    "\n",
    "최솟값 지점에 가까워질 수록 그레이디언트는 점점 0벡터에 가까워진다. \n",
    "따라서 그레이디언트가 충분히 작아지면 최솟값 지점에 매우 가깞다고 판단하여 그 위치에서\n",
    "최솟값의 근사치를 구하여 활용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Gradient-Descent.gif\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"\"https://github.com/pvigier/gradient-descent>pvigier: gradient-descent </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주의사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법은 지역 최솟값(local minimum)이 없고 전역 최솟값(global mininum)이 존재할 경우 유용하게 활용된다.\n",
    "반면에 지역 최솟값이 존재할 경우 제대로 작동하지 않을 수 있기 때문에 많은 주의를 요한다. \n",
    "아래 그림은 출발점과 이동 방향에 따라 도착 지점이 전역 또는 지역 최솟점이 될 수 있음을 잘 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Gradient.png\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 2: 그레이디언트 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단변수 함수와 다변수 함수의 경우 그레이디언트 계산이 조금 다르다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단변수 함수의 도함수 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f$가 단변수 함수(1차원 함수)일 때, 점 $x$에서의 그레이디언트는 다음과 같이 구한다. \n",
    "\n",
    "$$\n",
    "f'(x) = \\lim_h \\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "즉, $f'(x)$ 는 $x$가 아주 조금 변할 때 $f(x)$가 변하는 정도, 즉\n",
    "함수 $f$의 $x$에서의 **미분값**이 된다.\n",
    "이때 함수 $f'$ 을 함수 $f$의 **도함수**라 부른다.\n",
    "\n",
    "예를 들어, 제곱 함수 $f(x) = x^2$에 해당하는 `square()`가 아래와 같이 주어졌다고 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x: float) -> float:\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면 `square()`의 도함수 $f'(x) = 2x$는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(x: float) -> float:\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이와 같이 미분함수가 구체적으로 주어지는 경우는 많지 않다.\n",
    "따라서 여기서는 많은 경우 충분히 작은 $h$에 대한 함수의 변화율을 측정하면\n",
    "미분값의 근사치를 사용할 수 있다는 사실을 확인하고자 한다.\n",
    "\n",
    "아래 그림은 $h$가 작아질 때 \n",
    "두 점 $f(x+h)$ 와 $f(x)$ 지나는 직선이 변하는 과정을 보여준다.\n",
    "$h$가 0에 수렴하면 최종적으로 점 $x$에서의 접선이 되고\n",
    "미분값 $f'(x)$는 접선의 기울기가 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Derivative.gif\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 임의의 단변수 함수에 대해 함수 변화율을 구해주는 함수를 간단하게 구현한 것이다.\n",
    "          \n",
    "* `Callable`은 함수에 대한 자료형을 가리킨다. \n",
    "    * `Callable[[float], float]]`: 부동소수점을 하나 받아 부동소수점을 계산하는 함수들의 클래스를 가리킴.\n",
    "* `different_quotient()` 함수가 사용하는 세 인자와 리턴값의 자료형은 다음과 같다. \n",
    "    * 미분 대상 함수: `f: Callable[[float], float]`\n",
    "    * 미분 위치: `x: float`\n",
    "    * 인자가 변하는 정도: `h: float`\n",
    "    * 리턴값(`float`): $f'(x)$의 근사치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def difference_quotient(f: Callable[[float], float],\n",
    "                        x: float,\n",
    "                        h: float) -> float:\n",
    "    \"\"\"\n",
    "    함수 f의 x에서의 미분값 근사치 계산\n",
    "    f: 미분 대상 함수\n",
    "    x: 인자\n",
    "    h: x가 변하는 정도\n",
    "    \"\"\"\n",
    "    \n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 `square()`의 도함수 `derivative()` 를 `difference_quotient()`를 \n",
    "이용하여 충분히 근사적으로 구현할 수 있음을 그래프로 보여준다. \n",
    "근사치 계산을 위해 `h=0.001` 를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "h = 0.001\n",
    "xs = range(-10, 11)\n",
    "\n",
    "actuals = [derivative(x) for x in xs]\n",
    "estimates = [difference_quotient(square, x, h) for x in xs]\n",
    "\n",
    "plt.title(\"Actual Derivatives vs. Estimates\")\n",
    "# 실제 도함수 그래프(빨간색 점)\n",
    "plt.plot(xs, actuals, 'r.', label='Actual') \n",
    "# 근사치 그래프(검은색 +)\n",
    "plt.plot(xs, estimates, 'k+', label='Estimates')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다변수 함수의 그레이디언트 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다변수 함수의 그레이디언트는 매개변수 각가에 대한 **편도함수**(partial derivative)로\n",
    "구성된 벡터로 구성된다.\n",
    "예를 들어, $i$번째 편도함수는 $i$번째 매개변수를 제외한 다른 모든 매개변수를 고정하는 \n",
    "방식으로 계산된다. \n",
    "\n",
    "$f$가 다변수 함수(다차원 함수)일 때, 점 $\\mathbf{x}$에서의 $i$번째 도함수는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_i}f(\\mathbf x) = \\lim_h \\frac{f(\\mathbf{x}_h) - f(\\mathbf x)}{h}\n",
    "$$\n",
    "\n",
    "여기서 $\\mathbf{x}_h$는 $\\mathbf x$의 $i$번째 항목에 $h$를 더한 벡터를 가리킨다.\n",
    "즉, $\\frac{\\partial}{\\partial x_i} f(\\mathbf x)$ 는 $x_i$가 아주 조금 변할 때 \n",
    "$f(\\mathbf x)$가 변하는 정도, 즉\n",
    "함수 $f$의 $x$에서의 $i$번째 **편미분값**이 된다.\n",
    "이때 함수 $\\frac{\\partial}{\\partial x_i}f$ 를 함수 $f$의 $i$번째 **편도함수**라 부른다.\n",
    "\n",
    "아래 코드에서 정의된 `partial_difference_quotient`는 주어진 다변수 함수의 $i$번째\n",
    "편도함수의 근사치를 계산해주는 함수이다.\n",
    "사용되는 매개변수는 `difference_quotient()` 함수의 경우와 거의 같다.\n",
    "다만 `i`번째 편도함수의 근사치를 지정하기 위해 `i`번째 매개변수에만 `h`가 더해짐에 주의하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f: Callable[[LA.Vector], float],\n",
    "                                v: LA.Vector,\n",
    "                                i: int,\n",
    "                                h: float) -> float:\n",
    "    \"\"\"\n",
    "    함수 f의 v에서의 i번째 편미분값 근사치 계산\n",
    "    f: 편미분 대상 함수\n",
    "    v: 인자 벡터\n",
    "    i: i번째 인자를 가리킴\n",
    "    h: 인자 v_i가 변하는 정도\n",
    "    \"\"\"\n",
    "    \n",
    "    # v_i에 대해서만 h 더한 벡터\n",
    "    w = [v_j + (h if j == i else 0) for j, v_j in enumerate(v)]\n",
    "\n",
    "    return (f(w) - f(v)) / h    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 `estimate_gradient()` 함수는 편미분 근사치를 이용하여 \n",
    "그레이디언트의 근사치에 해당하는 벡터를 리스트로 계산한다. \n",
    "근사치 계산에 사용된 `h`의 기본값은 0.0001이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f: Callable[[LA.Vector], float],\n",
    "                      v: LA.Vector,\n",
    "                      h: float = 0.0001):\n",
    "    return [partial_difference_quotient(f, v, i, h) for i in range(len(v))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주의사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그레이디언트를 두 개의 함수값의 차이를 이용하여 근사치로 계산하는 방식은 계산 비용이 크다.\n",
    "벡터 `v`의 길이가 $n$이면 `estimate_gradient()` 함수를 호출할 때마다\n",
    "`f`를 $2n$ 번 호출해야 하기 때문이다. \n",
    "따라서 앞으로는 그레이디언트 함수가 수학적으로 쉽게 계산되는 경우만을 \n",
    "사용하여 경사하강법의 용도를 살펴볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 3: 경사하강법과 선형회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 정의한 제곱함수 `sum_of_squares()` 는 `v`가 0 벡터일 때 가장 작은 값을 갖는다. \n",
    "이 사실을 경사하강법을 이용하여 확인해보자. \n",
    "\n",
    "먼저, `sum_of_squares()` 함수의 그레이디언트는 다음과 같이 정의된다. \n",
    "\n",
    "$$\n",
    "\\nabla f(\\textbf{x}) =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial x_1} f(\\textbf{x}) \\\\\n",
    "    \\frac{\\partial}{\\partial x_2} f(\\textbf{x})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "아래 코드는 리스트를 이용하여 그레이디언트를 구현하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares_gradient(v: LA.Vector) -> LA.Vector:\n",
    "    return [2 * v_i for v_i in v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 임의의 지점(`v`)에서 계산된 그레이디언트에 스텝(step)이라는 특정 상수를 곱한 값을\n",
    "더해 새로운 지점을 계산하는 함수를 구현한다.\n",
    "\n",
    "* `add`, `scalar_multiply`, `distance` 등은 선형대수 부분에서 정의한 `linear_algebra` 모듈에서 가져온다.\n",
    "* `add(v, step)`: 스텝사이즈가 음수이면 그레이디언트가 가리키는 방향의 반대방향으로 지정된 비율만큼 움직인 벡터."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# v에서의 그레이디언트를 구한 후 스텝이 지정한 크기 비율과 방향으로 이동한 새로운 벡터 v'을 계산한다.\n",
    "def gradient_step(v: LA.Vector, gradient: LA.Vector, step_size: float) -> LA.Vector:\n",
    "    step = LA.scalar_multV(step_size, gradient)\n",
    "    return LA.addV(v, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 임의의 지점에서 출발하여 `gradient_step`을 충분히 반복하면\n",
    "제곱함수의 최솟점에 충분히 가깝게 근사할 수 있음을 아래 코드가 보여준다.\n",
    "실제로 1.0e-07보다 작다.\n",
    "\n",
    "* `random.seed(42)`: 실행할 때마다 동일한 결과를 보장해준다. 사용하지 않으면 매번 다른 결과가 나옴.\n",
    "* `grad`: 이동할 때마다 계산된 그레이디언트. 최종적으로 0 벡터에 가까운 값을 갖게 됨.\n",
    "* `if epoch%100 == 0`: 위치 이동을 100번 할 때마다 현재 위치 확인\n",
    "* `epoch`(에포크): 여기서는 그레이디언트 계산 횟수. 즉, 이동횟수를 가리킴.\n",
    "* `step_size=-0.01`: 그레이디언트 반대 방향, 즉, 최솟값 지점을 향해 이동할 때 사용되는 크기 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [2.732765249774521, -9.309789197635727, -4.409425359965263]\n",
      "100 [0.3624181137897112, -1.2346601088642208, -0.5847760329896551]\n",
      "200 [0.048063729299005625, -0.16374007531854073, -0.07755273779298358]\n",
      "300 [0.006374190434279768, -0.02171513607091833, -0.010285009644527733]\n",
      "400 [0.0008453423045827667, -0.002879851701919325, -0.0013639934114305214]\n",
      "500 [0.00011210892101281368, -0.00038192465375128993, -0.00018089220046728499]\n",
      "600 [1.4867835316559317e-05, -5.065067796575345e-05, -2.3989843290795995e-05]\n",
      "700 [1.971765716798422e-06, -6.717270417586384e-06, -3.1815223632100903e-06]\n",
      "800 [2.6149469369030636e-07, -8.908414196052704e-07, -4.2193208287814765e-07]\n",
      "900 [3.467931014604295e-08, -1.1814299344070243e-07, -5.5956445449048146e-08]\n",
      "\n",
      "----\n",
      "\n",
      "그레이디언트의 최종 값: [9.57758165411209e-09, -3.262822016281278e-08, -1.5453808714914104e-08]\n",
      "v의 최후 위치와 최솟점 사이의 거리: 1.830234305038648e-08\n"
     ]
    }
   ],
   "source": [
    "# 임의의 지점 선택\n",
    "random.seed(42)\n",
    "v = [random.uniform(-10, 10) for i in range(3)]\n",
    "\n",
    "# gradient_step 1000번 반복\n",
    "for epoch in range(1000):\n",
    "    grad = sum_of_squares_gradient(v)\n",
    "    v = gradient_step(v, grad, -0.01)\n",
    "    if epoch%100 == 0:\n",
    "        print(epoch, v)\n",
    "\n",
    "print(\"\\n----\\n\")        \n",
    "print(f\"그레이디언트의 최종 값: {grad}\")\n",
    "print(f\"v의 최후 위치와 최솟점 사이의 거리: {LA.distance(v, [0, 0, 0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 에포크와 스텝 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 사용된 코드에서 에포크(epoch)는 이동 횟수를 가리키며, \n",
    "에포크를 크게 하면 최솟값 지점에 보다 가까워진다.\n",
    "하지만 항상 수렴하는 방향으로 이동하는 것은 아니다.\n",
    "하지만 여기서는 스텝 크기를 너무 크게 지정하지만 않으면 항상 최솟값에 수렴하는 \n",
    "볼록함수만 다룬다. \n",
    "스텝 크기에 따른 수렴속도는 다음과 같다.\n",
    "\n",
    "* 스텝 크기 크게: 수렴 속도가 빨라진다.\n",
    "* 스텝 크기 작게: 수렴 속도가 느려진다.\n",
    "\n",
    "하지만 다루는 함수에 따른 적당한 스텝의 크기가 달라지며,\n",
    "보통 여러 실혐을 통해 정해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법을 이용하여 주어진 데이터들의 분포에 대한 선형 모델을 구하는 방법을\n",
    "**선형회귀**(linear regression)라 부른다. \n",
    "\n",
    "먼저 $y = f(x) = 20*x + 5$ 일차함수의 그래프에 해당하는 데이터를 구한다. \n",
    "여기서 $x$는 -0.5에서 0.5 사이에 있는 100개의 값으로 주어지며,\n",
    "$y$값에 약간의 잡음(가우시안 잡음)이 추가된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x는 -0.5에서 0.5 사이\n",
    "xs = [x/100 for x in range(-50, 50)]\n",
    "\n",
    "# 약간의 잡음 추가 (가우시안 잡음)\n",
    "error = [random.randrange(-100,100)/100 for _ in range(-50, 50)]\n",
    "\n",
    "# y = 20*x + 5 + 가우시안 잡음\n",
    "ys = [20*x + 5 + e for x, e in zip(xs, error)]\n",
    "\n",
    "# (x,y) 좌표값들의 리스트\n",
    "inputs = list(zip(xs, ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 프로그램은 잡음이 포함되어 직선으로 그려지지 않는 \n",
    "데이터의 분포를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xs, ys, 'r.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 위 그래프를 선형적으로 묘사하는 함수를 경사하강법을 이용하여 구현한다.\n",
    "구현 대상은 아래 모양의 함수이다. \n",
    "\n",
    "$$\n",
    "\\hat y = f(x) = \\theta_0 x + \\theta_1\n",
    "$$\n",
    "\n",
    "* $\\theta_0$: 직선의 기울기\n",
    "* $\\theta_1$: 절편\n",
    "* $\\hat y$: $y$에 대한 예측치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그래프를 가장 잘 묘사하는 직선을 구해야 한다.\n",
    "즉, 적절한 $\\theta_0$와 $\\theta_1$을 구해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기준"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법을 적용하려면 최소화 대상함수를 정해야 한다.\n",
    "여기서는 예측치 $\\hat y$와 실제 $y$ 사이의 오차의 **평균제곱오차**(mean squared error, MSE)를\n",
    "계산하는 함수를 사용한다.\n",
    "\n",
    "$$\n",
    "\\textrm{MSE}(\\theta_0, \\theta_1) = \\sum_{y \\in ys} (\\hat y - y)^2\n",
    "= \\sum_{x \\in \\textrm{xs}} (\\theta_0 x + \\theta_1 - y)^2\n",
    "$$\n",
    "\n",
    "즉, MSE를 최소로 하는 $\\theta_0, \\theta_1$을 구해야 한다.\n",
    "MSE의 그레이디언트는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_0} \\textrm{MSE}(\\theta_0, \\theta_1)\n",
    "= \\sum_{x \\in \\textrm{xs}} 2(\\hat y - y) x\\, ,\n",
    "\\qquad\n",
    "\\frac{\\partial}{\\partial \\theta_1} \\textrm{MSE}(\\theta_0, \\theta_1)\n",
    "= \\sum_{x \\in \\textrm{xs}} 2(\\hat y - y)\n",
    "$$\n",
    "\n",
    "아래 코드는 특정 x에 대한 실제 값 y와 예측치 $\\hat y$ 사이의 제곱오차와\n",
    "제곱오차의 그레이디언트를 계산한다.\n",
    "\n",
    "* `slope`: $\\theta_0$ (기울기)\n",
    "* `intercept`: $\\theta_1$ (절편)\n",
    "* `predicted`: $\\hat y$\n",
    "* `error`: $(\\hat y - y)$\n",
    "* `grad`: $(2(\\hat y - y) x, 2(\\hat y - y))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_gradient(x: float, y: float, theta: LA.Vector) -> LA.Vector:\n",
    "    # 기울기와 절편\n",
    "    slope, intercept = theta\n",
    "    # 예측치\n",
    "    predicted = slope * x + intercept\n",
    "    # 오차\n",
    "    error = (predicted - y)          \n",
    "    # 제곱 오차\n",
    "    squared_error = error ** 2       \n",
    "    # 특정 x에 대한 제곱오차의 그레이디언트 항목\n",
    "    grad = [2 * error * x, 2 * error]\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 임의의 $\\theta = (\\theta_0, \\theta_1)$로 시작한 후에\n",
    "경사하강법으로 평균제곱오차의 최솟값 지점을 구할 수 있다.\n",
    "\n",
    "* `vector_mean`: 벡터 항목들의 평균값 계산\n",
    "* `epoch`(에포크): 5000회 반복\n",
    "* `learning_rate`: 스텝 크기. 보통 **학습률**이라 부름.\n",
    "\n",
    "아래 코드에서 사용한 학습률은 0.001이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.5573631781037754, -0.3409606698035382]\n",
      "500 [1.0860605634068832, 2.9677764519232985]\n",
      "1000 [2.6080336786406573, 4.18874863946214]\n",
      "1500 [4.011976376077862, 4.642064339793808]\n",
      "2000 [5.305013852497387, 4.812897015495151]\n",
      "2500 [6.495163711496766, 4.879577951303121]\n",
      "3000 [7.5903398484398314, 4.90767120517886]\n",
      "3500 [8.598020889846392, 4.921296576290668]\n",
      "4000 [9.525160097963871, 4.929341055389218]\n",
      "4500 [10.378181475323187, 4.9350917239397925]\n",
      "최종 기울기: 11.161\n",
      "최종 절편: 4.940\n"
     ]
    }
   ],
   "source": [
    "from pydata06_linear_algebra_basics import vector_mean\n",
    "\n",
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(5000):\n",
    "    # 평균 제곱 오차 계산 (전체 훈련 데이터 대상)\n",
    "    grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "    # theta 값 업데이트. 그레이디언트 반대 방향으로 지정된 학습률 비율로 이동\n",
    "    theta = gradient_step(theta, grad, -learning_rate)\n",
    "    # 500번에 한 번 학습과정 확인\n",
    "    if epoch % 500 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 최종 기울기가 11.398로 애초에 사용한 20과 차이가 크다.\n",
    "이유는 학습률이 너무 낮아서 5000번의 에포크가 충북한 학습을 위해 부족했기 때문이다.\n",
    "이에 대한 해결책은 보통 두 가지이다. \n",
    "\n",
    "* 첫째: 학습률 키우기\n",
    "* 둘째: 에포크 키우기\n",
    "\n",
    "아래 코드는 먼저 에포크를 네 배 늘린 결과를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.539845326196926, -0.8765658904200774]\n",
      "1000 [3.5347058588398195, 4.120657569741477]\n",
      "2000 [6.089146831484904, 4.807295221174362]\n",
      "3000 [8.254060613242245, 4.909957251324375]\n",
      "4000 [10.08698653375398, 4.932225333575831]\n",
      "5000 [11.63858380675949, 4.942345701376079]\n",
      "6000 [12.951998593477466, 4.949733516900191]\n",
      "7000 [14.063789219509484, 4.955827987429306]\n",
      "8000 [15.0049066769235, 4.960965378519356]\n",
      "9000 [15.801551256517898, 4.965311213690506]\n",
      "10000 [16.475901275838954, 4.968989518433547]\n",
      "11000 [17.046730424738943, 4.972103106006536]\n",
      "12000 [17.529930406587045, 4.974738713135767]\n",
      "13000 [17.938953356610103, 4.976969721779518]\n",
      "14000 [18.285186344872766, 4.978858243526626]\n",
      "15000 [18.57826838869039, 4.980456854364114]\n",
      "16000 [18.826358799830466, 4.981810059132508]\n",
      "17000 [19.036364337180647, 4.982955530628767]\n",
      "18000 [19.21413148874078, 4.983925158418778]\n",
      "19000 [19.36460923600656, 4.984745936634814]\n",
      "최종 기울기: 19.492\n",
      "최종 절편: 4.985\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(20000):\n",
    "    # 평균 제곱 오차 계산 (전체 훈련 데이터 대상)\n",
    "    grad = LA.vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "    # theta 값 업데이트. 그레이디언트 반대 방향으로 지정된 학습률 비율로 이동\n",
    "    theta = gradient_step(theta, grad, -learning_rate)\n",
    "    # 1000번에 한 번 학습과정 확인\n",
    "    if epoch % 1000 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반면에 아래 코드는 학습률을 0.01로 키웠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.6757583873253522, 0.6957201300303815]\n",
      "500 [11.706690051576645, 4.942804228398626]\n",
      "1000 [16.50780846999213, 4.969163560245711]\n",
      "1500 [18.593137756203685, 4.98053795906695]\n",
      "2000 [19.498884502023184, 4.9854783387029045]\n",
      "2500 [19.89228863870891, 4.98762415462158]\n",
      "3000 [20.0631607014304, 4.988556173271237]\n",
      "3500 [20.137377668401463, 4.988960988407408]\n",
      "4000 [20.16961323750889, 4.9891368167500385]\n",
      "4500 [20.18361450915995, 4.9892131864390965]\n",
      "최종 기울기: 20.190\n",
      "최종 절편: 4.989\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(5000):\n",
    "    # 평균 제곱 오차 계산 (전체 훈련 데이터 대상)\n",
    "    grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "    # theta 값 업데이트. 그레이디언트 반대 방향으로 지정된 학습률 비율로 이동\n",
    "    theta = gradient_step(theta, grad, -learning_rate)\n",
    "    # 500번에 한 번 학습과정 확인\n",
    "    if epoch % 500 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과를 비교했을 때 학습률을 키우는 것이 보다 효과적이다. \n",
    "최종적으로 구해진 기울기와 절편을 이용하여 처음에 주어진 데이터의 분포를\n",
    "선형적으로 학습한 1차함수의 그래프는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 예측치\n",
    "zs = [slope *x + intercept for x in xs]\n",
    "\n",
    "# 실제 데이터 분포\n",
    "plt.plot(xs, ys, 'r.', label='Actuals')\n",
    "# 예측치 그래프\n",
    "plt.plot(xs, zs, 'b-', label='Estimates')\n",
    "\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 4: 미니배치/확률적 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 사용한 경사하강법은 주어진 데이터셋 전체를 대상으로 평균제곱오차와 그레이디언트를 계산하였다.\n",
    "이런 방식을 **배치 경사하강법**이라 부른다.\n",
    "\n",
    "**주의**: 배치(batch)는 원래 하나의 묶음을 나타내지만 여기서는 주어진 (훈련) 데이터셋 전체를 가리키는\n",
    "의미로 사용된다.\n",
    "\n",
    "그런데 사용된 데이터셋의 크기가 100이었기 때문에 계산이 별로 오래 걸리지 않았지만,\n",
    "데이터셋이 커지면 그러한 계산이 매우 오래 걸릴 수 있다.\n",
    "실전에서 사용되는 데이터셋의 크기는 몇 만에서 수십억까지 다양하며, \n",
    "그런 경우에 적절한 학습률을 찾는 과정이 매우 오래 걸릴 수 있다.\n",
    "\n",
    "데이터셋이 매우 큰 경우에는 따라서 아래 두 가지 방식을 추천한다.\n",
    "\n",
    "* 미니배치 경사하강법\n",
    "* 확률적 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미니배치 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 경사하강법과는 달리 미니매치 경사하강법(mini-batch gradient descent)은\n",
    "일정 크기의 훈련 데이터를 대상으로 그레이디언트를 계산한다.\n",
    "\n",
    "예를 들어, 전체 데이터셋의 크기가 1000이고 미니배치의 크기를 10이라 하면,\n",
    "배치 경사하강법에서는 하나의 에포크를 돌 때마다 한 번 MSE와 그레이디언트를 계산하였지만\n",
    "미니배치 경사하강법에서는 10개의 데이터를 확인할 때마다 MSE와 그레이디언트를 계산하여\n",
    "하나의 에포크를 돌 때마다 총 100번 기울기와 절편을 업데이트한다. \n",
    "\n",
    "아래 코드에서 정의된 `minibatches()` 함수는 호출될 때마다\n",
    "`batch_size`로 지정된 크기의 데이터 세트를 전체 데이터셋에서\n",
    "선택해서 내준다.\n",
    "\n",
    "데이터를 선택하는 방식은 다음과 같다.\n",
    "\n",
    "* `minibatches()` 함수는 제너레이터이다. \n",
    "    즉, 요구될 때마다 항목을 생성하여 리턴하는 함수이다. \n",
    "* 전체 데이터셋을 인덱스 기준으로 미니배치 크기(`batch_size`) 만큼씩 구분하여\n",
    "    `batch_starts` 리스트에 저장한다.\n",
    "* 섞기(`shuffle`) 옵션이 `True`일 경우, `batch_starts`에 포함된 항목들의 순서를 무작위로 섞는다.\n",
    "* 최종적으로 `batch_starts` 에 포함된 인데스를 기준으로 지정된 미니배치 크기만큼의 \n",
    "    데이터를 다음 MSE, 그레이디언트 계산용으로 내어 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Iterator\n",
    "\n",
    "# 제너레이터 함수 정의\n",
    "def minibatches(dataset: List[float],\n",
    "                batch_size: int,\n",
    "                shuffle: bool = True) -> Iterator[List[float]]:\n",
    "    \"\"\"\n",
    "    dataset: 전체 데이터셋\n",
    "    batch_size: 미니배치 크기\n",
    "    shuffle: 섞기 옵션\n",
    "    리턴값: 이터레이터\n",
    "    \"\"\"\n",
    "\n",
    "    # 0번 인덱스부터 시작하여, batch_size 배수 번째에 해당하는 인덱스만 선택\n",
    "    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
    "    \n",
    "    # shuffle 옵션이 참이면 인덱스 섞기\n",
    "    if shuffle: random.shuffle(batch_starts)\n",
    "\n",
    "    # batch_starts에  포함된 인덱스를 기준으로 해서 미니배치 크기만큼씩 선택해서 \n",
    "    # 다음 MSE와 그레이디언트 계산에 필요한 훈련 데이터 세트를 지정함.\n",
    "    for start in batch_starts:\n",
    "        end = start + batch_size\n",
    "        yield dataset[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 미니배치 경사하강법을 이전에 사용했던 데이터에 대해 적용한다.\n",
    "학습률(`learning_rate`)을 0.001로 하면 학습이 제대로 이루어지지 않는다.\n",
    "에포크 수를 키우거나 합습률을 크게 해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.18096617572366355, -0.810253155496382]\n",
      "100 [1.4325823295978932, 2.7965832049687727]\n",
      "200 [2.9277291181808662, 4.127583024520555]\n",
      "300 [4.307256114408177, 4.6201205624228345]\n",
      "400 [5.577858834315078, 4.805372329280058]\n",
      "500 [6.747297927532496, 4.877973082222024]\n",
      "600 [7.823358363323769, 4.9075106536945095]\n",
      "700 [8.813363542511828, 4.9227014341159645]\n",
      "800 [9.724173605813512, 4.930098309332715]\n",
      "900 [10.562099366999867, 4.935898279024058]\n",
      "최종 기울기: 11.326\n",
      "최종 절편: 4.940\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률 지정\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 1000번의 에포크\n",
    "for epoch in range(1000):\n",
    "    # 미니배치의 크기를 20으로 지정함\n",
    "    # 따라서 한 번의 에포크마다 5번 MSE와 그레이디언트 계산 후 기울기와 절편 업데이트\n",
    "    # 섞기 옵션 사용\n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    # 100개의 에포크가 지날 때마다 학습 내용 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률을 0.01로 키우면 좋은 결과가 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1.0089547406154515, 0.9789387260040882]\n",
      "100 [11.920008658960944, 4.952894140709578]\n",
      "200 [16.630466145513115, 4.970879458498564]\n",
      "300 [18.65959750842576, 4.983489508744]\n",
      "400 [19.533544758414724, 4.986114887570471]\n",
      "500 [19.910159234650248, 4.9876175552247615]\n",
      "600 [20.072120246973697, 4.988277830931272]\n",
      "700 [20.141813699267466, 4.989044796751943]\n",
      "800 [20.171950036505514, 4.989673862726055]\n",
      "900 [20.18490980813245, 4.988704425280094]\n",
      "최종 기울기: 20.190\n",
      "최종 절편: 4.989\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률 지정\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 1000번의 에포크\n",
    "for epoch in range(1000):\n",
    "    # 미니배치의 크기를 20으로 지정함\n",
    "    # 따라서 한 번의 에포크마다 5번 MSE와 그레이디언트 계산 후 기울기와 절편 업데이트\n",
    "    # 섞기 옵션 사용\n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    # 100개의 에포크가 지날 때마다 학습 내용 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률을 0.01로 두고 에포크를 3000으로 늘려도 성능이 그렇게 좋아지지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.818987279044814, 0.5496759496355055]\n",
      "100 [11.132254675200233, 4.944402547449223]\n",
      "200 [16.29128890751974, 4.967275599762484]\n",
      "300 [18.51315652813586, 4.978364613062517]\n",
      "400 [19.470399111844596, 4.984337308055089]\n",
      "500 [19.882668827278895, 4.988157388454654]\n",
      "600 [20.060140900004065, 4.987966154755475]\n",
      "700 [20.136885552244106, 4.987828522542775]\n",
      "800 [20.16977510712028, 4.988976100905835]\n",
      "900 [20.183962972393914, 4.989098169073801]\n",
      "1000 [20.19002718033645, 4.989306092251666]\n",
      "1100 [20.19268900836928, 4.989013649684174]\n",
      "1200 [20.193941008879644, 4.98931327368746]\n",
      "1300 [20.194440751083505, 4.988662935916931]\n",
      "1400 [20.194676502572356, 4.989409414992472]\n",
      "1500 [20.19501205890768, 4.989279358430207]\n",
      "1600 [20.194844639671665, 4.9895094830280255]\n",
      "1700 [20.195038900604747, 4.989915517219994]\n",
      "1800 [20.195104991373057, 4.988570101328778]\n",
      "1900 [20.195111383724566, 4.989401145459436]\n",
      "2000 [20.194820820191822, 4.988704721391355]\n",
      "2100 [20.194956704659234, 4.989706094313501]\n",
      "2200 [20.194893907702024, 4.989209975413154]\n",
      "2300 [20.19482708931829, 4.989765252131663]\n",
      "2400 [20.19497162191007, 4.990361301343332]\n",
      "2500 [20.19504959681753, 4.989715152995273]\n",
      "2600 [20.195037903439133, 4.989885179868943]\n",
      "2700 [20.195048938410093, 4.989683544147567]\n",
      "2800 [20.194794217945287, 4.988956630961846]\n",
      "2900 [20.19472935840392, 4.9896646884885545]\n",
      "최종 기울기: 20.194\n",
      "최종 절편: 4.989\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률 지정\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 3000번의 에포크\n",
    "for epoch in range(3000):\n",
    "    # 미니배치의 크기를 20으로 지정함\n",
    "    # 따라서 한 번의 에포크마다 5번 MSE와 그레이디언트 계산 후 기울기와 절편 업데이트\n",
    "    # 섞기 옵션 사용\n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    # 100개의 에포크가 지날 때마다 학습 내용 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 과정을 살펴보면 기울기가 19.966 정도에서 더 이상 좋아지지 않는다.\n",
    "이런 경우 특별히 더 좋은 결과를 얻을 수 없다는 것을 의미한다.\n",
    "사실, 데이터셋을 지정할 때 가우시안 잡음을 추가하였기에 완벽한 선형함수를 찾는 것은 애초부터 불가능하다.\n",
    "따라서 위 결과를 미니배치 경사하강법을 사용한 선형회귀로 얻을 수 있는 최선으로 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 확률적 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률적 경사하강법(stochastic gradient descent, SGD)은\n",
    "미니배치의 크기가 1인 미니배치 경사하강법을 가리킨다.\n",
    "즉, 하나의 데이터를 학습할 때마다 그레이디언트를 계산하여 기울기와 절편을 업데이트 한다. \n",
    "\n",
    "아래 코드는 주어진 데이터셋을 대상로 SGD를 적용하는 방식을 보여준다.\n",
    "학습률을 0.001로 했음에도 불구하여 1000번의 에포크를 반복한 후에 이전에\n",
    "0.01을 사용했던 경우와 거의 동일한 결과를 얻는다는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.15554810709612274, 1.509287732512692]\n",
      "100 [16.342595331414632, 5.034998314265212]\n",
      "200 [19.472338990801568, 4.997992330602081]\n",
      "300 [20.06303054508488, 4.991008012136133]\n",
      "400 [20.174514599453477, 4.989689828163571]\n",
      "500 [20.19555552053715, 4.989441040970666]\n",
      "600 [20.199526674641238, 4.989394086168721]\n",
      "700 [20.200276169630307, 4.98938522416341]\n",
      "800 [20.200417625419696, 4.9893835515945835]\n",
      "900 [20.200444323050316, 4.989383235922631]\n",
      "최종 기울기: 20.200\n",
      "최종 절편: 4.989\n"
     ]
    }
   ],
   "source": [
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 에포크는 1000\n",
    "for epoch in range(1000):\n",
    "    for x, y in inputs:\n",
    "        grad = linear_gradient(x, y, theta)\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률을 0.01로 하면 오히려 결과가 나빠진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [2.259915006214208, 6.8246196207535865]\n",
      "100 [20.261030045940156, 4.990193171642614]\n",
      "200 [20.26103222379059, 4.990192825899716]\n",
      "300 [20.26103222379092, 4.990192825899663]\n",
      "400 [20.26103222379092, 4.990192825899663]\n",
      "500 [20.26103222379092, 4.990192825899663]\n",
      "600 [20.26103222379092, 4.990192825899663]\n",
      "700 [20.26103222379092, 4.990192825899663]\n",
      "800 [20.26103222379092, 4.990192825899663]\n",
      "900 [20.26103222379092, 4.990192825899663]\n",
      "최종 기울기: 20.261\n",
      "최종 절편: 4.990\n"
     ]
    }
   ],
   "source": [
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 에포크는 1000\n",
    "for epoch in range(1000):\n",
    "    for x, y in inputs:\n",
    "        grad = linear_gradient(x, y, theta)\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 살펴본 것에 따르면 확률적 경사하강법의 성능이 가장 좋았다.\n",
    "하지만 이것은 경우에 따라 다르다.\n",
    "배치 경사하강법, 미니배치 경사하강법, 확률적 경사하강법 각각의 장단점이 있지만\n",
    "여기서는 더 이상 자세히 다루지 않는다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
