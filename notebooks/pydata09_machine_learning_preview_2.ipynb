{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 머신러닝 맛보기 2편"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__참고:__ 조엘 그루스의 [<밑바닥부터 시작하는 데이터과학(2판)>](https://github.com/joelgrus/data-science-from-scratch)에 포함된 \n",
    "[Gradient Descent](https://github.com/joelgrus/data-science-from-scratch/blob/master/scratch/gradient_descent.py)의 소스코드 일부를 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주요 내용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 경사하강법 의미\n",
    "1. 그레이디언트 계산\n",
    "1. 경사하강법으로 최솟점 구하기\n",
    "1. 경사하강법과 선형회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 필수 모듈 불러오기\n",
    "- 그래프 출력 관련 기본 설정 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 노트북 실행 결과를 동일하게 유지하기 위해\n",
    "np.random.seed(42)\n",
    "\n",
    "# 깔끔한 그래프 출력을 위해\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주의사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "연습문제를 제외한 본문은 넘파이를 전혀 사용하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 준비사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형대수에서 정의한 함수를 사용하기 위한 준비가 필요하며\n",
    "필요한 코드가 `pydata06_linear_algebra_basics.py` 파일에 저장되어 있다고 가정한다.\n",
    "\n",
    "먼저 파일을 다운로드한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pydata06_linear_algebra_basics.py',\n",
       " <http.client.HTTPMessage at 0x7fae25084610>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "file_url = \"https://raw.githubusercontent.com/codingalzi/pydata/master/notebooks/pydata06_linear_algebra_basics.py\"\n",
    "file_name = \"pydata06_linear_algebra_basics.py\"\n",
    "urllib.request.urlretrieve(file_url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydata06_linear_algebra_basics as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 데이터셋을 가장 잘 반영하는 최적의 수학적 모델을 찾으려 할 때 가장 기본적으로 사용되는 기법이\n",
    "**경사하강법**(gradient descent)이다.\n",
    "최적의 모델에 대한 기준은 학습법에 따라 다르지만, \n",
    "보통 학습된 모델의 오류를 최소화하도록 유도하는 기준을 사용한다. \n",
    "\n",
    "여기서는 선형회귀 모델을 학습하는 데에 기본적으로 사용되는 **평균 제곱 오차**(mean squared error, MSE)를\n",
    "최소화하기 위해 경사하강법을 적용하는 과정을 살펴본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 기본 아이디어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 $f:\\mathbf{R}^n \\to \\mathbf{R}$의 최솟값 지점을 구하고자 한다.\n",
    "예를 들어, \n",
    "길이가 $n$인 실수 벡터를 입력받아 항목들의 제곱의 합을 계산하는 함수가 다음과 같다고 하자.\n",
    "\n",
    "$$\n",
    "f(\\mathbf v) = f(v_1, ..., v_n) = \\sum_{k=1}^{n} v_k^2 = v_1^2 + \\cdots v_n^2\n",
    "$$\n",
    "\n",
    "아래 코드에서 정의된 `sum_of_squares()`가 위 함수를 파이썬으로 구현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__참고:__ `LA.Vector` 는 `typing.List[float]`, 즉 부동소수점들의 리스트 자료형을 가리킴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(v: LA.Vector) -> float:\n",
    "    \"\"\"\n",
    "    v 벡터에 포함된 원소들의 제곱의 합 계산\n",
    "    \"\"\"\n",
    "    return LA.dot(v, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 `sum_of_squares(v)`가 최대 또는 최소가 되는 벡터 `v`를 찾고자 한다.\n",
    "\n",
    "그런데 특정 함수의 최솟값이 존재한다는 것이 알려졌다 하더라도 실제로 최솟값\n",
    "지점을 확인하는 일은 일반적으로 매우 어렵고, 경우에 따라 불가능하다. \n",
    "따라서 보통 해당 함수의 그래프 위에 존재하는 임의의 점에서 시작한 후\n",
    "그레이디언트 반대 방향으로 조금씩 이동하면서 최솟값 지점을 찾아가는\n",
    "**경사하강법**(gradient descent)을 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그레이디언트의 정의와 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 $f:\\mathbf{R}^n \\to \\mathbf{R}$가 점 $\\mathbf{v}\\in \\mathbf{R}^n$에서 \n",
    "미분 가능할 때 $\\mathbf{v}$에서 $f$의 __그레이디언트__는 다음처럼 편미분으로 이루어진 벡터로 정의된다. \n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{v}) =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial}{\\partial v_1} f(\\mathbf{v}) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial}{\\partial v_n} f(\\mathbf{v})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "아래에서 왼편 그림은 $n=1$인 경우 2차원 상에서, 오른편 그림은 $n=2$인 경우에 3차원 상에서 \n",
    "그려지는 함수의 그래프와 특정 지점에서의 \n",
    "그레이디언트를 보여주고 있다. \n",
    "\n",
    "* 왼편 그림\n",
    "    * 그레이디언트는 접선(tangent line)의 기울기(slope)를 가리키는 미분값 $f'(x)$이다.\n",
    "    * 갈색 직선이 접선을 가리킨다.\n",
    "* 오른편 그림\n",
    "    * 그레이디언트는 편미분값으로 구성된 길이가 2인 벡터\n",
    "        $\\left( \\frac{\\partial}{\\partial v_1} f(\\mathbf{v}), \\frac{\\partial}{\\partial v_2} f(\\mathbf{v}) \\right)$ 로 계산되며, 위쪽으로 향하는 파란색 화살표로 표시된다.\n",
    "    * 파란색 초평면(hyperplane)은 해당 지점에서 그래프와 접하는 평면을 보여준다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"https://raw.githubusercontent.com/codingalzi/pydata/master/notebooks/images/Tangent-line.png\" alt=\"경사하강법\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"https://raw.githubusercontent.com/codingalzi/pydata/master/notebooks/images/tangent_space-90.png\" alt=\"경사하강법\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            &#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>\n",
    "        </td>\n",
    "        <td>\n",
    "            &#60;출처\t&#62; <a href=\"\"https://github.com/pvigier/gradient-descent>pvigier: gradient-descent </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 경사하강법 작동 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법은 다음 과정을 반복하여 최솟값 지점을 찾아가는 것을 의미한다. \n",
    "\n",
    "* 해당 지점에서 그레이디언트(gradient)를 계산한다.\n",
    "* 계산된 그레이디언트의 방향(반대방향)으로 그레이디언트 크기의 일정 비율만큼 이동한다. \n",
    "\n",
    "아래 그림은 2차원 함수의 최솟값 지점을 경사하강법으로 찾아가는 과정을 보여준다.\n",
    "최솟값 지점은 해당 지점에서 구한 그레이디언트의 반대방향으로 조금씩 이동하는 방식으로 이루어진다. \n",
    "\n",
    "최솟값 지점에 가까워질 수록 그레이디언트는 점점 0벡터에 가까워진다. \n",
    "따라서 그레이디언트가 충분히 작아지면 최솟값 지점에 매우 가깞다고 판단하여 그 위치에서\n",
    "최솟값 지점의 근사치를 구하여 활용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/codingalzi/pydata/master/notebooks/images/Gradient-Descent.gif\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"\"https://github.com/pvigier/gradient-descent>pvigier: gradient-descent </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주의사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법은 지역 최솟값(local minimum)이 없고 전역 최솟값(global mininum)이 존재할 경우 유용하게 활용된다.\n",
    "반면에 지역 최솟값이 존재할 경우 제대로 작동하지 않을 수 있기 때문에 많은 주의를 요한다. \n",
    "아래 그림은 출발점과 이동 방향에 따라 도착 지점이 전역 또는 지역 최솟점이 될 수 있음을 잘 보여준다.\n",
    "하지만 이런 경우는 여기서 다루지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/codingalzi/pydata/master/notebooks/images/Gradient.png\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 파이썬으로 그레이디언트 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단변수 함수와 다변수 함수의 경우 그레이디언트 계산이 조금 다르다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단변수 함수의 도함수 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f$가 단변수 함수(1차원 함수)일 때, 점 $x$에서의 그레이디언트는 다음과 같이 구한다. \n",
    "\n",
    "$$\n",
    "f'(x) = \\lim_{h\\to 0} \\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "즉, $f'(x)$ 는 $x$가 아주 조금 변할 때 $f(x)$가 변하는 정도, 즉\n",
    "함수 $f$의 $x$에서의 **미분값**이 된다.\n",
    "이때 함수 $f'$ 을 함수 $f$의 **도함수**라 부른다.\n",
    "\n",
    "예를 들어, 제곱 함수 $f(x) = x^2$에 해당하는 `square()`가 아래와 같이 주어졌다고 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x: float) -> float:\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면 $f$의 도함수 $f'(x) = 2x$는 아래 `square_prime()` 함수로 구현된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_prime(x: float) -> float:\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 도함수 근사치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이와 같이 미분함수가 구체적으로 주어지는 경우는 일반적이지 않다.\n",
    "따라서 여기서는 충분히 작은 $h$에 대한 함수의 변화율\n",
    "\n",
    "$$\n",
    "\\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "을 측정하여\n",
    "미분값의 근사치로 사용한다.\n",
    "\n",
    "아래 그림은 $h$가 작아질 때 \n",
    "두 점 $f(x+h)$ 와 $f(x)$ 지나는 직선이 변하는 과정을 보여준다.\n",
    "$h$가 0에 수렴하면 최종적으로 점 $x$에서의 접선이 되고\n",
    "미분값 $f'(x)$는 접선의 기울기가 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/codingalzi/pydata/master/notebooks/images/Derivative.gif\" alt=\"경사하강법\">\n",
    "&#60;출처\t&#62; <a href=\"https://en.wikipedia.org/wiki/Derivative\">위키:미분</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 임의의 단변수 함수에 대해 함수 변화율을 구해주는 함수를 간단하게 구현한 것이다.\n",
    "          \n",
    "* `Callable`은 함수에 대한 자료형을 가리킨다. \n",
    "    * `Callable[[float], float]`: 부동소수점을 하나 받아 부동소수점을 계산하는 함수들의 클래스를 가리킴.\n",
    "* `different_quotient()` 함수가 사용하는 세 인자와 리턴값의 자료형은 다음과 같다. \n",
    "    * 미분 대상 함수: `f: Callable[[float], float]`\n",
    "    * 미분 위치: `x: float`\n",
    "    * 인자가 변하는 정도: `h: float`\n",
    "    * 리턴값(`float`): $f'(x)$의 근사치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def difference_quotient(f: Callable[[float], float],\n",
    "                        x: float,\n",
    "                        h: float) -> float:\n",
    "    \"\"\"\n",
    "    함수 f의 x에서의 미분값 근사치 계산\n",
    "    f: 미분 대상 함수\n",
    "    x: 인자\n",
    "    h: x가 변하는 정도\n",
    "    \"\"\"\n",
    "    \n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 `square()`의 도함수 `square_prime()` 을 `difference_quotient()`를 \n",
    "이용하여 충분히 근사적으로 구현할 수 있음을 보여준다. \n",
    "근사치 계산을 위해 `h=0.001` 를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = 0.001\n",
    "xs = range(-10, 11)\n",
    "\n",
    "actuals = [square_prime(x) for x in xs]\n",
    "estimates = [difference_quotient(square, x, h) for x in xs]\n",
    "\n",
    "plt.title(\"Actual Derivatives vs. Estimates\")\n",
    "\n",
    "# 실제 도함수 그래프(빨간색 직선)\n",
    "plt.plot(xs, actuals, 'r-', label='square_prime') \n",
    "\n",
    "# 근사치 그래프(검은색 점)\n",
    "plt.plot(xs, estimates, 'k.', label='Estimates')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다변수 함수의 그레이디언트 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다변수 함수의 그레이디언트는 매개변수 각가에 대한 **편도함수**(partial derivative)로\n",
    "구성된 벡터로 구성된다.\n",
    "예를 들어, $i$번째 편도함수는 $i$번째 매개변수를 제외한 다른 모든 매개변수를 고정하는 \n",
    "방식으로 계산된다. \n",
    "\n",
    "$f$가 다변수 함수(다차원 함수)일 때, 점 $\\mathbf{v}$에서의 $i$번째 도함수는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial v_i}f(\\mathbf{v}) = \\lim_{h \\to 0} \\frac{f(\\mathbf{v}_{i, h}) - f(\\mathbf x)}{h}\n",
    "$$\n",
    "\n",
    "여기서 $\\mathbf{v}_{i, h}$는 $\\mathbf{v}$의 $i$번째 항목에 $h$를 더한 벡터\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_{i, h} = (v_1, \\dots, v_{i-1}, v_i + h, v_{i+1}, \\dots, v_n)\n",
    "$$\n",
    "\n",
    "를 가리킨다.\n",
    "즉, $\\frac{\\partial}{\\partial v_i} f(\\mathbf{v})$ 는 $v_i$가 아주 조금 변할 때 \n",
    "$f(\\mathbf{v})$가 변하는 정도, 즉\n",
    "함수 $f$의 $\\mathbf{v}$에서의 $i$번째 **편미분값**이 된다.\n",
    "이때 함수 $\\frac{\\partial}{\\partial v_i}f$ 를 함수 $f$의 $i$번째 **편도함수**라 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 편도함수 근사치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드에서 정의된 `partial_difference_quotient()`는 주어진 다변수 함수의 $i$번째\n",
    "편도함수의 근사치를 계산해주는 함수이다.\n",
    "사용되는 매개변수는 `difference_quotient()` 함수의 경우와 거의 같다.\n",
    "다만 $i$번째 편도함수의 근사치를 지정하기 위해 $i$번째 매개변수에만 $h$가 더해짐에 주의하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f: Callable[[LA.Vector], float],\n",
    "                                v: LA.Vector,\n",
    "                                i: int,\n",
    "                                h: float) -> float:\n",
    "    \"\"\"\n",
    "    함수 f의 v에서의 i번째 편미분값 근사치 계산\n",
    "    f: 편미분 대상 함수\n",
    "    v: 인자 벡터\n",
    "    i: i번째 인자를 가리킴\n",
    "    h: 인자 v_i가 변하는 정도\n",
    "    \"\"\"\n",
    "    \n",
    "    # v_i에 대해서만 h를 더한 벡터\n",
    "    w = [v_j + (h if j == i else 0) for j, v_j in enumerate(v)]\n",
    "\n",
    "    return (f(w) - f(v)) / h    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 `estimate_gradient()` 함수는 편미분 근사치를 이용하여 \n",
    "그레이디언트의 근사치에 해당하는 벡터\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\frac{f(\\mathbf{v}_{1, h}) - f(\\mathbf x)}{h}\\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{f(\\mathbf{v}_{n, h}) - f(\\mathbf x)}{h}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "를 리스트로 계산한다. \n",
    "근사치 계산에 사용된 $h$의 기본값은 0.0001이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f: Callable[[LA.Vector], float],\n",
    "                      v: LA.Vector,\n",
    "                      h: float = 0.0001):\n",
    "    \n",
    "    return [partial_difference_quotient(f, v, i, h) for i in range(len(v))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__참고:__ \n",
    "그레이디언트를 두 개의 함수값의 차이를 이용하여 근사치로 계산하는 방식은 계산 비용이 크다.\n",
    "벡터 `v`의 길이가 $n$이면 `estimate_gradient()` 함수를 호출할 때마다\n",
    "`f`를 $2n$ 번 호출해야 하기 때문이다. \n",
    "따라서 앞으로는 그레이디언트 함수가 수학적으로 쉽게 계산되는 경우만을 \n",
    "사용하여 경사하강법의 용도를 살펴볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 경사하강법으로 최솟점 구하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 정의한 제곱함수 `sum_of_squares()` 는 `v`가 0 벡터일 때 가장 작은 값을 갖는다. \n",
    "이 사실을 경사하강법을 이용하여 확인해보자. \n",
    "\n",
    "먼저, `sum_of_squares()` 함수의 그레이디언트는 다음과 같이 정의된다. \n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{v}) =\n",
    "\\begin{bmatrix}\n",
    "    2v_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    2v_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "아래 코드는 리스트를 이용하여 그레이디언트를 구현하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares_gradient(v: LA.Vector) -> LA.Vector:\n",
    "    return [2 * v_i for v_i in v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 임의의 지점(`v`)에서 계산된 그레이디언트에 학습률(learning rate)이라는 특정 상수를 곱한 값을\n",
    "빼서 새로운 지점을 계산하는 함수를 구현한다.\n",
    "\n",
    "__참고:__ `subtractV`, `scalar_multV`, `distance` 등은 선형대수 부분에서 정의한 \n",
    "`pydata06_linear_algebra_basics.py` 모듈에서 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def gradient_step(v: LA.Vector, gradient: LA.Vector, learning_rate: float) -> LA.Vector:\n",
    "    step = LA.scalar_multV(learning_rate, gradient)\n",
    "    new_V = LA.subtractV(v, step)                    # 그레이디언트의 반대방향으로 이동\n",
    "\n",
    "    return new_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 임의의 지점에서 출발하여 `gradient_step()` 함수를 충분히 반복하면\n",
    "제곱함수의 최솟점에 충분히 가깝게 근사할 수 있음을 아래 코드가 보여준다.\n",
    "실제로 전역 최솟값 위치인 원점에 매우 가깝게 접근한다.\n",
    "\n",
    "* `random.seed(42)`: 실행할 때마다 동일한 결과를 보장해준다. 사용하지 않으면 매번 다른 결과가 나옴.\n",
    "* `grad`: 이동할 때마다 계산된 그레이디언트. 최종적으로 0 벡터에 가까운 값을 갖게 됨.\n",
    "* `if epoch%100 == 0`: 위치 이동을 100번 할 때마다 현재 위치 확인\n",
    "* `epoch`(에포크): 여기서는 그레이디언트 계산 횟수. 즉, 이동횟수를 가리킴.\n",
    "* `learning_rate=0.01`: 그레이디언트 반대 방향, 즉, 최솟값 지점을 향해 이동할 때 사용되는 크기 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [2.732765249774521, -9.309789197635727, -4.409425359965263]\n",
      "100 [0.3624181137897112, -1.2346601088642208, -0.5847760329896551]\n",
      "200 [0.048063729299005625, -0.16374007531854073, -0.07755273779298358]\n",
      "300 [0.006374190434279768, -0.02171513607091833, -0.010285009644527733]\n",
      "400 [0.0008453423045827667, -0.002879851701919325, -0.0013639934114305214]\n",
      "500 [0.00011210892101281368, -0.00038192465375128993, -0.00018089220046728499]\n",
      "600 [1.4867835316559317e-05, -5.065067796575345e-05, -2.3989843290795995e-05]\n",
      "700 [1.971765716798422e-06, -6.717270417586384e-06, -3.1815223632100903e-06]\n",
      "800 [2.6149469369030636e-07, -8.908414196052704e-07, -4.2193208287814765e-07]\n",
      "900 [3.467931014604295e-08, -1.1814299344070243e-07, -5.5956445449048146e-08]\n",
      "\n",
      "----\n",
      "\n",
      "그레이디언트의 최종 값: [9.57758165411209e-09, -3.262822016281278e-08, -1.5453808714914104e-08]\n",
      "v의 최후 위치와 최솟점 사이의 거리: 1.830234305038648e-08\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# 임의로 선택된 출발점 좌표\n",
    "v = [random.uniform(-10, 10) for i in range(3)]\n",
    "\n",
    "# gradient_step 1000번 반복\n",
    "for epoch in range(1000):\n",
    "    grad = sum_of_squares_gradient(v)  # 그레이디언트 계산\n",
    "    \n",
    "    v = gradient_step(v, grad, 0.01)   # 좌표 업데이트\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        print(epoch, v)\n",
    "\n",
    "print(\"\\n----\\n\")        \n",
    "print(f\"그레이디언트의 최종 값: {grad}\")\n",
    "print(f\"v의 최후 위치와 최솟점 사이의 거리: {LA.distance(v, [0, 0, 0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 에포크와 학습률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 사용된 코드에서 에포크(epoch)는 이동 횟수를 가리키며, \n",
    "에포크를 크게 하면 최솟값 지점에 보다 가까워진다.\n",
    "하지만 항상 수렴하는 방향으로 이동하는 것은 아니다.\n",
    "하지만 여기서는 학습률을 너무 크게 지정하지만 않으면 항상 최솟값 지점에 수렴하는 \n",
    "볼록함수만 다룬다. \n",
    "학습률에 따른 수렴속도는 다음과 같다.\n",
    "\n",
    "* 학습률 크게: 수렴 속도가 빨라진다.\n",
    "* 학습률 작게: 수렴 속도가 느려진다.\n",
    "\n",
    "하지만 다루는 함수에 따른 적당한 학습률이 달라지며,\n",
    "보통 여러 실혐을 통해 정해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 경사하강법과 선형회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 데이터들의 분포에 대한 선형회귀 모델의 훈련과정을 구현해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 세트 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 $y = f(x) = 5 + 20x$ 일차함수의 그래프에 해당하는 데이터를 구한다. \n",
    "여기서 $x$ 는 -0.5에서 0.5 사이에 있는 100개의 값으로 주어지며,\n",
    "$y$ 값에 약간의 잡음(가우시안 잡음)이 추가된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x는 -0.5에서 0.5 사이\n",
    "X = [x/100 for x in range(-50, 50)]\n",
    "\n",
    "# 약간의 잡음 추가 (가우시안 잡음)\n",
    "error = [random.randrange(-100,100)/100 for _ in range(-50, 50)]\n",
    "\n",
    "# y = 5 + 20*x + 가우시안 잡음\n",
    "y = [5 + 20*x + e for x, e in zip(X, error)]\n",
    "\n",
    "# (x,y) 좌표값들의 리스트\n",
    "inputs = list(zip(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 프로그램은 잡음이 포함되어 직선으로 그려지지 않는 \n",
    "데이터의 분포를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X, y, 'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 위 그래프를 선형적으로 묘사하는 함수를 경사하강법을 이용하여 구현한다.\n",
    "즉 아래 1차 함수의 직선 그래프가 위 데이터의 분포를 최적으로 묘사하는 \n",
    "$\\theta_0$와 $\\theta_1$을 구해야 한다.\n",
    "\n",
    "$$\n",
    "\\hat y_x = \\theta_0 + \\theta_1\\cdot x\n",
    "$$\n",
    "\n",
    "* $\\theta_0$: 절편\n",
    "* $\\theta_1$: 기울기\n",
    "* $\\hat y_x$: $y_x$ 에 대한 예측값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비용함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법을 적용하려면 최적의 기준을 지정해야 한다. \n",
    "여기서는 예측치 $\\hat y$와 실제 $y$ 사이의 **평균제곱오차**(mean squared error, MSE)를\n",
    "최소로 하는 것을 최적의 기준으로 사용한다($m$은 $X$의 원소 개수).\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\textrm{MSE}(\\theta_0, \\theta_1) \n",
    "&= \\frac{1}{m} \\sum_{x \\in X} (\\hat y_x - y_x)^2\\\\[1ex] \n",
    "&= \\frac{1}{100}\\sum_{x \\in X} (\\theta_0 + \\theta_1\\cdot x - y_x)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "즉, MSE를 최소로 하는 $\\theta_0, \\theta_1$을 구해야 한다.\n",
    "($x, y_x$는 모두 상수임에 주의하라.)\n",
    "이처럼 최적의 기준으로 최솟값 지점을 찾는 대상인 함수를 __비용함수__라 부른다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 비용함수의 그레이언트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE의 그레이디언트는 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\nabla \\textrm{MSE}(\\theta_0, \\theta_1)\n",
    "= \n",
    "\\frac{1}{100} \\cdot\n",
    "\\begin{bmatrix}\n",
    "\\sum_{x \\in X} 2(\\hat y_x - y_x) \\\\[1ex]\n",
    "\\sum_{x \\in X} 2(\\hat y_x - y_x) x\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "이처럼 전체 데이터셋을 대상으로 평균제곱오차의 그레이디언트를 활용하는 경사하강법을\n",
    "__배치 경사하강법__이라 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드의 `linear_gradient()` 함수는 특정 데이터 샘플 $x$에 대해 타깃(실제값) $y$와 예측치 $\\hat y$ 사이의 \n",
    "MSE 함수의 그레이디언트를 계산한다.\n",
    "\n",
    "| 변수 | 의미 |\n",
    "| :--- | :--- |\n",
    "| intercept | $\\theta_0$ (절편) |\n",
    "| slope | $\\theta_1$ (기울기) |\n",
    "| predicted | $\\hat y$ (예측값) |\n",
    "| error | $\\hat y - y$ (오차) |\n",
    "| grad | $x$에 대한 제곱오차의 그레이디언트 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_gradient(x: float, y_x: float, theta: LA.Vector) -> LA.Vector:\n",
    "    \n",
    "    intercept, slope = theta           \n",
    "    predicted = intercept + slope * x  \n",
    "    error = (predicted - y_x)            \n",
    "    grad = [2 * error, 2 * error * x]  \n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평균제곱오차는 이제 아래와 같이 지정된다. \n",
    "\n",
    "```python\n",
    "LA.vector_mean([linear_gradient(x, y_x, theta) for x, y_x in list(zip(X, y))])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE() 함수의 최솟값 지점 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 임의의 $\\theta = (\\theta_0, \\theta_1)$로 시작한 후에\n",
    "경사하강법으로 평균제곱오차의 최솟값 지점을 구할 수 있다.\n",
    "\n",
    "* `vector_mean`: 벡터 항목들의 평균값 계산\n",
    "* `epoch`(에포크): 5000회 반복. MSE를 5000번 계산.\n",
    "* `learning_rate`: 학습률\n",
    "\n",
    "아래 코드에서 사용한 학습률은 0.001이다.\n",
    "\n",
    "__참고:__ __에포크__는 훈련 세트에 포함된 전체 훈련 샘플에 대해 예측값을 계산하는 것을 의미한다.\n",
    "따라서 아래 코드는 MSE를 총 5,000번 계산할 때마다 $\\theta_0, \\theta_1$을 업데이트 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.5498750271006548, -0.348065046729531]\n",
      "500 [2.8916275137318697, 1.2779917858845682]\n",
      "1000 [4.161340996373953, 2.78438692444515]\n",
      "1500 [4.63252303459113, 4.174144289893471]\n",
      "2000 [4.809879153546342, 5.454184661194932]\n",
      "2500 [4.878918399660604, 6.6323964362145]\n",
      "3000 [4.907842395609395, 7.716596269085061]\n",
      "3500 [4.921739999085596, 8.714181270348421]\n",
      "4000 [4.929854101282013, 9.632032625512535]\n",
      "4500 [4.935602366135786, 10.476509096849366]\n",
      "최종 기울기: 11.252\n",
      "최종 절편: 4.940\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(5000):\n",
    "    # 평균 제곱 오차 계산\n",
    "    grad = LA.vector_mean([linear_gradient(x, y_x, theta) for x, y_x in inputs])\n",
    "    \n",
    "    # theta 값 업데이트.\n",
    "    theta = gradient_step(theta, grad, learning_rate)\n",
    "    \n",
    "    # 500번에 한 번 학습과정 확인\n",
    "    if epoch % 500 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 최종 기울기가 11.xx 정도로 애초에 사용한 20과 차이가 크다.\n",
    "이유는 학습률이 너무 낮아서 5000번의 에포크가 충북한 학습을 위해 부족했기 때문이다.\n",
    "이에 대한 해결책은 보통 두 가지이다. \n",
    "\n",
    "* 첫째: 학습률 키우기\n",
    "* 둘째: 에포크 키우기\n",
    "\n",
    "아래 코드는 먼저 에포크를 네 배 늘린 결과를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.5453216196333258, -0.8846541803415932]\n",
      "1000 [4.3071961393797, 2.3344318287971526]\n",
      "2000 [4.827835026420881, 5.073878601499392]\n",
      "3000 [4.908792115820234, 7.394749506518404]\n",
      "4000 [4.928733502968913, 9.359603724571762]\n",
      "5000 [4.939051515502755, 11.022864887660349]\n",
      "6000 [4.9468993258866565, 12.430800212519703]\n",
      "7000 [4.953422706517003, 13.62260111714172]\n",
      "8000 [4.958928503638361, 14.631446280141436]\n",
      "9000 [4.96358691111091, 15.485421540772457]\n",
      "10000 [4.967529901914979, 16.208301288794544]\n",
      "11000 [4.97086755619471, 16.82021026697407]\n",
      "12000 [4.973692834861904, 17.338183828260746]\n",
      "13000 [4.976084398412164, 17.776642193387854]\n",
      "14000 [4.97810882798133, 18.147791905104715]\n",
      "15000 [4.979822483153499, 18.46196565445812]\n",
      "16000 [4.9812730715626445, 18.727909939652793]\n",
      "17000 [4.982500977134955, 18.95302856580554]\n",
      "18000 [4.98354038437498, 19.14358876454719]\n",
      "19000 [4.98442023005287, 19.30489567177664]\n",
      "최종 기울기: 19.441\n",
      "최종 절편: 4.985\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(20000):\n",
    "    # 평균 제곱 오차 계산 (전체 훈련 데이터 대상)\n",
    "    grad = LA.vector_mean([linear_gradient(x, y_x, theta) for x, y_x in inputs])\n",
    "    \n",
    "    # theta 값 업데이트. 그레이디언트 반대 방향으로 지정된 학습률 만큼 이동\n",
    "    theta = gradient_step(theta, grad, learning_rate)\n",
    "    \n",
    "    # 1000번에 한 번 학습과정 확인\n",
    "    if epoch % 1000 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반면에 아래 코드는 학습률을 0.01로 키웠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.7285600934134986, 0.6423040193614589]\n",
      "500 [4.942726752245414, 11.692237628170421]\n",
      "1000 [4.9691293209021055, 16.501531174187775]\n",
      "1500 [4.980523087447129, 18.590411261023977]\n",
      "2000 [4.985471879329149, 19.497700269800106]\n",
      "2500 [4.987621349042239, 19.891774276545842]\n",
      "3000 [4.988554954689417, 20.062937292178695]\n",
      "3500 [4.988960459125764, 20.137280632310304]\n",
      "4000 [4.989136586860627, 20.16957109062855]\n",
      "4500 [4.989213086588395, 20.183596202986426]\n",
      "최종 기울기: 20.190\n",
      "최종 절편: 4.989\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(5000):\n",
    "    # 평균 제곱 오차 계산 (전체 훈련 데이터 대상)\n",
    "    grad = LA.vector_mean([linear_gradient(x, y_x, theta) for x, y_x in inputs])\n",
    "    \n",
    "    # theta 값 업데이트. 그레이디언트 반대 방향으로 지정된 학습률 만큼 이동\n",
    "    theta = gradient_step(theta, grad, learning_rate)\n",
    "    \n",
    "    # 500번에 한 번 학습과정 확인\n",
    "    if epoch % 500 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과를 비교했을 때 학습률을 키우는 것이 보다 효과적이다. \n",
    "최종적으로 구해진 기울기와 절편을 이용하여 처음에 주어진 데이터의 분포를\n",
    "선형적으로 학습한 1차함수의 그래프는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 예측치\n",
    "y_hat = [intercept + slope * x for x in X]\n",
    "\n",
    "# 실제 데이터 분포\n",
    "plt.plot(X, y, 'k.', label='Actuals')\n",
    "# 예측치 그래프\n",
    "plt.plot(X, y_hat, 'r-', label='Estimates')\n",
    "\n",
    "plt.title(\"Linear Regression\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 배치/미니배치/확률적 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 사용한 배치 경사하강법은 주어진 데이터셋 전체를 대상으로 평균제곱오차의 그레이디언트를 계산하였다.\n",
    "이런 방식을 **배치 경사하강법**이라 부른다.\n",
    "\n",
    "**주의**: 배치(batch)는 원래 하나의 묶음을 나타내지만 여기서는 주어진 (훈련) 데이터셋 전체를 가리키는\n",
    "의미로 사용된다.\n",
    "\n",
    "그런데 사용된 데이터셋의 크기가 100이었기 때문에 계산이 별로 오래 걸리지 않았지만,\n",
    "데이터셋이 커지면 그러한 계산이 매우 오래 걸릴 수 있다.\n",
    "실전에서 사용되는 데이터셋의 크기는 몇 만에서 수십억까지 다양하며, \n",
    "그런 경우에 적절한 학습률을 찾는 과정이 매우 오래 걸릴 수 있다.\n",
    "\n",
    "데이터셋이 매우 큰 경우에는 따라서 아래 두 가지 방식 중 하나를 사용할 것을 추천한다.\n",
    "\n",
    "* 미니배치 경사하강법\n",
    "* 확률적 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미니배치 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니매치 경사하강법(mini-batch gradient descent)은\n",
    "전체 훈련 세트를 쪼갠 여러 개의 미니배치(작은 훈련 세트)에 대해 평균제곱오차의 그레이디언트를\n",
    "계산하여 $\\theta_0, \\theta_1, \\dots $ 를 업데이트한다. \n",
    "\n",
    "예를 들어, 전체 데이터셋의 크기가 1000이고 미니배치의 크기를 10이라 하면,\n",
    "배치 경사하강법에서는 하나의 에포크를 돌 때마다 한 번 MSE와 그레이디언트를 계산하였지만\n",
    "미니배치 경사하강법에서는 10개의 데이터를 확인할 때마다 MSE와 그레이디언트를 계산한다.\n",
    "즉,하나의 에포크를 돌 때마다 총 100번 기울기와 절편을 업데이트한다. \n",
    "\n",
    "아래 코드에서 정의된 `minibatches()` 함수는 호출될 때마다\n",
    "`batch_size`로 지정된 크기의 데이터 세트를 전체 데이터셋에서\n",
    "선택해서 내준다.\n",
    "\n",
    "데이터를 선택하는 방식은 다음과 같다.\n",
    "\n",
    "* `minibatches()`: 호출될 때마다 `batch_size` 크기 만큼의 훈련 데이터를 생성해서 제공하는 제너레이터이다.\n",
    "* `batch_starts`: 전체 데이터셋에서 차례대로 선택할 인덱스로 이루어진 어레이.\n",
    "* 섞기(`shuffle`) 옵션: `batch_starts`에 포함된 항목들의 순서를 무작위로 섞을 때 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Iterator\n",
    "\n",
    "# 제너레이터 함수 정의\n",
    "def minibatches(dataset: List[float],\n",
    "                batch_size: int,\n",
    "                shuffle: bool = True) -> Iterator[List[float]]:\n",
    "    \"\"\"\n",
    "    dataset: 전체 데이터셋\n",
    "    batch_size: 미니배치 크기\n",
    "    shuffle: 섞기 옵션\n",
    "    리턴값: 이터레이터\n",
    "    \"\"\"\n",
    "\n",
    "    # 0번 인덱스부터 시작하여, batch_size 배수 번째에 해당하는 인덱스만 선택\n",
    "    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
    "    \n",
    "    # shuffle 옵션이 참이면 인덱스 섞기\n",
    "    if shuffle: random.shuffle(batch_starts)\n",
    "\n",
    "    # batch_starts에  포함된 인덱스를 기준으로 해서 미니배치 크기만큼씩 선택해서 \n",
    "    # 다음 MSE와 그레이디언트 계산에 필요한 훈련 데이터 세트를 지정함.\n",
    "    for start in batch_starts:\n",
    "        end = start + batch_size\n",
    "        yield dataset[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 미니배치 경사하강법을 이전에 사용했던 데이터에 대해 적용한다.\n",
    "학습률(`learning_rate`)을 0.001로 하면 학습이 제대로 이루어지지 않는다.\n",
    "에포크 수를 키우거나 합습률을 크게 해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.14700601161032584, -0.850324957636322]\n",
      "100 [3.0383003005334195, 0.81879593562066]\n",
      "200 [4.21458258079334, 2.363808765349366]\n",
      "300 [4.65037766647808, 3.7887387104243513]\n",
      "400 [4.814916905651929, 5.100937311842601]\n",
      "500 [4.880054707558669, 6.308581062740375]\n",
      "600 [4.906941797916403, 7.419765388918784]\n",
      "700 [4.921300749140654, 8.442074919728974]\n",
      "800 [4.928451522266928, 9.382601070352939]\n",
      "900 [4.934261548296817, 10.247863490712117]\n",
      "최종 기울기: 11.036\n",
      "최종 절편: 4.939\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률 지정\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 1000번의 에포크\n",
    "for epoch in range(1000):\n",
    "    # 미니배치의 크기를 20으로 지정함\n",
    "    # 따라서 한 번의 에포크마다 5번 MSE와 그레이디언트 계산 후 기울기와 절편 업데이트\n",
    "    \n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = LA.vector_mean([linear_gradient(x, y_x, theta) for x, y_x in batch])\n",
    "        theta = gradient_step(theta, grad, learning_rate)\n",
    "\n",
    "    # 100개의 에포크가 지날 때마다 학습 내용 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률을 0.01로 키우면 좋은 결과가 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1.237825277922001, 0.7249192271332124]\n",
      "100 [4.952368308578804, 11.798388090708]\n",
      "200 [4.970608256534244, 16.578080917985734]\n",
      "300 [4.983392717759511, 18.637033407981054]\n",
      "400 [4.986056813379833, 19.523825735584676]\n",
      "500 [4.987595092175629, 19.90597277649338]\n",
      "600 [4.988266095521663, 20.070316968833907]\n",
      "700 [4.989040195003638, 20.14103697686222]\n",
      "800 [4.989672228772879, 20.17161547560229]\n",
      "900 [4.988703699118148, 20.184765698131514]\n",
      "최종 기울기: 20.190\n",
      "최종 절편: 4.989\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률 지정\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 1000번의 에포크\n",
    "for epoch in range(1000):\n",
    "    # 미니배치의 크기를 20으로 지정함\n",
    "    # 따라서 한 번의 에포크마다 5번 MSE와 그레이디언트 계산 후 기울기와 절편 업데이트\n",
    "\n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = LA.vector_mean([linear_gradient(x, y_x, theta) for x, y_x in batch])\n",
    "        theta = gradient_step(theta, grad, learning_rate)\n",
    "    \n",
    "    # 100개의 에포크가 지날 때마다 학습 내용 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률을 0.01로 두고 에포크를 3000으로 늘려도 성능이 그렇게 좋아지지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.43549987222961334, 0.26254473969499653]\n",
      "300 [4.978902536543744, 18.599116202912136]\n",
      "600 [4.988010821486811, 20.067010548132913]\n",
      "900 [4.989101560704833, 20.18451199354063]\n",
      "1200 [4.989313445216165, 20.193984886037637]\n",
      "1500 [4.98927937941379, 20.195015565259936]\n",
      "1800 [4.988570103335046, 20.195105271575724]\n",
      "2100 [4.989706094445955, 20.194956727051277]\n",
      "2400 [4.990361301352804, 20.194971623699445]\n",
      "2700 [4.989683544148361, 20.19504893855309]\n",
      "최종 기울기: 20.194\n",
      "최종 절편: 4.989\n"
     ]
    }
   ],
   "source": [
    "# 임의의 기울기와 절편으로 시작\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률 지정\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 1000번의 에포크\n",
    "for epoch in range(3000):\n",
    "    # 미니배치의 크기를 20으로 지정함\n",
    "    # 따라서 한 번의 에포크마다 5번 MSE와 그레이디언트 계산 후 기울기와 절편 업데이트\n",
    "    \n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = LA.vector_mean([linear_gradient(x, y_x, theta) for x, y_x in batch])\n",
    "        theta = gradient_step(theta, grad, learning_rate)\n",
    "    \n",
    "    # 100개의 에포크가 지날 때마다 학습 내용 출력\n",
    "    if epoch % 300 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 과정을 살펴보면 기울기가 20.195 정도에서 더 이상 좋아지지 않는다.\n",
    "이런 경우 특별히 더 좋은 결과를 얻을 수 없다는 것을 의미한다.\n",
    "사실, 데이터셋을 지정할 때 가우시안 잡음을 추가하였기에 완벽한 선형함수를 찾는 것은 애초부터 불가능하다.\n",
    "따라서 위 결과를 미니배치 경사하강법을 사용한 선형회귀로 얻을 수 있는 최선으로 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 확률적 경사하강법(SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률적 경사하강법(stochastic gradient descent, SGD)은\n",
    "미니배치의 크기가 1인 미니배치 경사하강법을 가리킨다.\n",
    "즉, 하나의 데이터를 학습할 때마다 그레이디언트를 계산하여 기울기와 절편을 업데이트 한다. \n",
    "\n",
    "아래 코드는 주어진 데이터셋을 대상로 SGD를 적용하는 방식을 보여준다.\n",
    "학습률을 0.001로 했음에도 불구하여 1000번의 에포크를 반복한 후에 이전에\n",
    "0.01을 사용했던 경우와 거의 동일한 결과를 얻는다는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.550382182232026, 0.9895536437047407]\n",
      "100 [5.032490886155949, 16.554658325069674]\n",
      "200 [4.997519092010245, 19.51236265820966]\n",
      "300 [4.990918695543546, 20.070584404084553]\n",
      "400 [4.989672971014022, 20.175940275546683]\n",
      "500 [4.9894378594398585, 20.19582459523417]\n",
      "600 [4.9893934857031255, 20.19957745840122]\n",
      "700 [4.989385110834649, 20.20028575429328]\n",
      "800 [4.989383530205495, 20.200419434379114]\n",
      "900 [4.989383231885758, 20.20044466446389]\n",
      "최종 기울기: 20.200\n",
      "최종 절편: 4.989\n"
     ]
    }
   ],
   "source": [
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 에포크는 1000\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    # 매 훈련 샘플에 대해 예측값을 확인한 후 바로 theta 값 업데이트\n",
    "    for x, y_x in inputs:\n",
    "        grad = linear_gradient(x, y_x, theta)\n",
    "        theta = gradient_step(theta, grad, learning_rate)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률을 0.01로 하면 결과가 오히려 좀 더 나빠진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [6.8116974087303825, 2.3020744483619793]\n",
      "100 [4.990193170861476, 20.26103005086055]\n",
      "200 [4.990192825899716, 20.26103222379059]\n",
      "300 [4.990192825899663, 20.26103222379092]\n",
      "400 [4.990192825899663, 20.26103222379092]\n",
      "500 [4.990192825899663, 20.26103222379092]\n",
      "600 [4.990192825899663, 20.26103222379092]\n",
      "700 [4.990192825899663, 20.26103222379092]\n",
      "800 [4.990192825899663, 20.26103222379092]\n",
      "900 [4.990192825899663, 20.26103222379092]\n",
      "최종 기울기: 20.261\n",
      "최종 절편: 4.990\n"
     ]
    }
   ],
   "source": [
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "# 학습률\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 에포크는 1000\n",
    "for epoch in range(1000):\n",
    "    for x, y_x in inputs:\n",
    "        grad = linear_gradient(x, y_x, theta)\n",
    "        theta = gradient_step(theta, grad, learning_rate)\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "intercept, slope = theta\n",
    "\n",
    "print(f\"최종 기울기: {slope:.3f}\")\n",
    "print(f\"최종 절편: {intercept:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 살펴본 것에 따르면 확률적 경사하강법의 성능이 가장 좋았다.\n",
    "하지만 이것은 경우에 따라 다르다.\n",
    "아래 그림은 각각의 방식에서 절편과 기울기가 학습되는 과정을 잘 보여준다.\n",
    "\n",
    "- 배치학습: 수렴값에 진동 없이 가장 빠르게 접근.\n",
    "- 미니배치 학습: 중간 속도로 접근함. 수렴값 근처에서 진동이 어느 정도 있음.\n",
    "- 확률적 경사 하강법: 매우 변화가 심함. 극한값 근처에서 여전히 진동이 매우 심함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/pydata/master/notebooks/images/gradient_descent_comparison.png\" width=\"500\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사이킷런의 확률적 경사하강법 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "linreg_sgd = linear_model.SGDRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런의 머신러닝 모델은 기본적으로 훈련 세트를 2차원 어레이로 요구하기에\n",
    "아래처럼 먼저 차원을 추가한다.\n",
    "\n",
    "__주의사항:__ 타깃 어레이 `y`는 1차원 어레이어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[X]\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사이킷런 모델의 훈련방법은 `fit()` 메서드를 호출하는 방식으로 이루어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg_sgd.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련결과로 찾아낸 선형회귀 모델의 절편과 기울기는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "절편:\t 4.982221986296401\n",
      "기울기:\t 18.840822703467992\n"
     ]
    }
   ],
   "source": [
    "t0, t1 = linreg_sgd.intercept_[0], linreg_sgd.coef_[0]\n",
    "\n",
    "print(f\"절편:\\t {t0}\")\n",
    "print(f\"기울기:\\t {t1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기에 대한 훈련이 부족해보인다. \n",
    "확률적 경사하강법을 사용하기에 수렴값 근처에서 많은 진동이 발생할 수 있기에 학습률을 보다 작게 해보자.\n",
    "\n",
    "* `eta0`: 학습률 기정. 기본값은 0.01임.\n",
    "\n",
    "아래 코드는 학습률을 0.001로 지정한 후에 훈련한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "절편:\t 4.927561524163672\n",
      "기울기:\t 9.365537800747957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gslee/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:1208: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    }
   ],
   "source": [
    "linreg_sgd_1 = linear_model.SGDRegressor(eta0=0.001)\n",
    "linreg_sgd_1.fit(X, y)\n",
    "\n",
    "t0, t1 = linreg_sgd_1.intercept_[0], linreg_sgd_1.coef_[0]\n",
    "\n",
    "print(f\"절편:\\t {t0}\")\n",
    "print(f\"기울기:\\t {t1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기가 훨씬 덜 학습된 것으로 보인다. 즉, 학습률이 너무 적다.\n",
    "그리고 경고창에서 알 수 있듯이 반복 훈련이 덜 되었다. 반복훈련의 기본값은 1,000번이다.\n",
    "이를 5,000번으로 올려보자. \n",
    "\n",
    "* `max_iter`: 반복훈련 횟수 지정. 기본값은 1,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "절편:\t 4.95832372425737\n",
      "기울기:\t 14.503989381717352\n"
     ]
    }
   ],
   "source": [
    "linreg_sgd_1 = linear_model.SGDRegressor(eta0=0.001, max_iter=5000)\n",
    "linreg_sgd_1.fit(X, y)\n",
    "\n",
    "t0, t1 = linreg_sgd_1.intercept_[0], linreg_sgd_1.coef_[0]\n",
    "\n",
    "print(f\"절편:\\t {t0}\")\n",
    "print(f\"기울기:\\t {t1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경고는 발생하지 않지만 여전히 훈련이 덜 되어 있다. 아무래도 반복횟수의 문제는 아닌 것 같다.\n",
    "실제로 반복횟수를 50,000으로 올려도 별 변화가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "절편:\t 4.95823344002572\n",
      "기울기:\t 14.50321304652143\n"
     ]
    }
   ],
   "source": [
    "linreg_sgd_1 = linear_model.SGDRegressor(eta0=0.001, max_iter=50000)\n",
    "linreg_sgd_1.fit(X, y)\n",
    "\n",
    "t0, t1 = linreg_sgd_1.intercept_[0], linreg_sgd_1.coef_[0]\n",
    "\n",
    "print(f\"절편:\\t {t0}\")\n",
    "print(f\"기울기:\\t {t1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결론적으로 학습률 0.001이 너무 작다. 따라서 학습률을 기본값인 0.01보다 좀 더 크게 해보자.\n",
    "아래 코드는 0.03을 학습률로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "절편:\t 4.983589984969054\n",
      "기울기:\t 19.604285712686572\n"
     ]
    }
   ],
   "source": [
    "linreg_sgd_1 = linear_model.SGDRegressor(eta0=0.03)\n",
    "linreg_sgd_1.fit(X, y)\n",
    "\n",
    "t0, t1 = linreg_sgd_1.intercept_[0], linreg_sgd_1.coef_[0]\n",
    "\n",
    "print(f\"절편:\\t {t0}\")\n",
    "print(f\"기울기:\\t {t1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반복훈련 횟수를 늘려도 별 변화가 없다.\n",
    "앞서 언급한 대로 확률적 경사하강법은 수렴값 근처에서 심하게 진동하기 때문이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "절편:\t 4.985259928226336\n",
      "기울기:\t 19.597986815558308\n"
     ]
    }
   ],
   "source": [
    "linreg_sgd_1 = linear_model.SGDRegressor(eta0=0.03, max_iter=10000)\n",
    "linreg_sgd_1.fit(X, y)\n",
    "\n",
    "t0, t1 = linreg_sgd_1.intercept_[0], linreg_sgd_1.coef_[0]\n",
    "\n",
    "print(f\"절편:\\t {t0}\")\n",
    "print(f\"기울기:\\t {t1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "넘파이 어레이를 활용하여 선형회귀 모델을 배치 경사하강법으로 구현하려 한다.\n",
    "즉, 본문과는 달리 어레이 메서드를 다양하게 활용해야 한다.\n",
    "\n",
    "먼저, 아래 코드는 100개의 샘플을 임의로 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 2 * np.random.rand(100)\n",
    "y = 4 + 3 * X + np.random.randn(100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 1\n",
    "\n",
    "산점도를 그리는 코드를 완성하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pass 부분을 적절한 코드로 대체하라.\n",
    "\n",
    "pass\n",
    "\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)             \n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)   \n",
    "plt.axis([0, 2, 0, 15])                      \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gradient_step()` 함수는 매 에포크마다\n",
    "절편과 기울기를 \n",
    "그레이디언트를 이용하여 지정된 학습율의 비율만큼 업데이트 한다.\n",
    "넘파이 어레이를 활용한 연산임에 주의해야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(v, gradient, learning_rate):\n",
    "    step = gradient*learning_rate\n",
    "    new_V = v - step\n",
    "\n",
    "    return new_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 2\n",
    "\n",
    "`linear_gradient()` 함수가 매 에포크마다 그레이디언트를 \n",
    "계산하도록 아래 코드를 완성하라.\n",
    "단, 넘파이 어레이와 관련된 함수를 이용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass와 None 각각을 적절한 코드와 값으로 대체하라.\n",
    "def linear_gradient(X, y, theta):\n",
    "    \n",
    "    intercept, slope = theta[0], theta[1]           \n",
    "    predicted = intercept + slope * X  \n",
    "    error = (predicted - y) \n",
    "    pass\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 3\n",
    "\n",
    "배치 경사하강법으로 선형회귀 모델의 절편과 기울기를 구하는 코드를 작성하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass문을 적절한 코드로 대체하여 배치 경사하강법을 구현하라.\n",
    "\n",
    "theta = np.random.uniform(-1, 1, 2)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 4\n",
    "\n",
    "학습률과 에포크를 어느 정도로 하는 것이 적당해 보이는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 5\n",
    "\n",
    "데이터셋 `X`와 `y`에 대해 미니배치 경사하강법을 이용하여 선형회귀 모델의 절편과 기울기를 구하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass문을 적절한 코드로 대체하여 미니배치 경사하강법을 구현하라.\n",
    "\n",
    "theta = np.random.uniform(-1, 1, 2)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 6\n",
    "\n",
    "데이터셋 `X`와 `y`에 대해 확률적 경사하강법을 이용하여 선형회귀 모델의 절편과 기울기를 구하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass문을 적절한 코드로 대체하여 확률적 경사하강법을 구현하라.\n",
    "\n",
    "theta = np.random.uniform(-1, 1, 2)\n",
    "\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
